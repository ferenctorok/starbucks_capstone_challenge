{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook\n",
    "\n",
    "This notebook contains the training code for the Starbucks Capstone Challenge.\n",
    "\n",
    "It has got the following structure:\n",
    "* Checking the correlation of the features in the training data\n",
    "* Based on this seleceting the sufficient features\n",
    "* Shuffleing the data\n",
    "* splitting the data into train, validation and test sets\n",
    "* evaluating the benchmark model kNN\n",
    "* Training a feed forward neural network for classification with 4 classes:\n",
    "    * creating data loaders\n",
    "    * defining the model\n",
    "    * finding sufficient hyperparameters\n",
    "    * training the models\n",
    "    * evaluating the trained model\n",
    "* Extra experiment: Training a feed forward neural network for binary classification:\n",
    "    * creating the binary labels from the 4 original classes\n",
    "    * creating data loaders\n",
    "    * defining the model\n",
    "    * finding sufficient hyperparameters\n",
    "    * training the models\n",
    "    * evaluating the trained model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from importlib import reload\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from source import training_helpers\n",
    "from source import model\n",
    "from source.model import Linear_NN\n",
    "from source import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is avalilabe\n"
     ]
    }
   ],
   "source": [
    "# setting up torch device:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('cuda is avalilabe' if torch.cuda.is_available() else 'cuda is NOT avaliable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(training_helpers)\n",
    "reload(model)\n",
    "reload(solver)\n",
    "from source.model import Linear_NN\n",
    "from source.solver import NN_Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the dataset and checking feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read training data from data/training_data_standardized.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>U</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>membership_length</th>\n",
       "      <th>av_money_spent</th>\n",
       "      <th>num_received</th>\n",
       "      <th>viewed/received</th>\n",
       "      <th>...</th>\n",
       "      <th>offer_4</th>\n",
       "      <th>offer_5</th>\n",
       "      <th>offer_6</th>\n",
       "      <th>offer_7</th>\n",
       "      <th>offer_8</th>\n",
       "      <th>offer_9</th>\n",
       "      <th>informational</th>\n",
       "      <th>bogo</th>\n",
       "      <th>discount</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.193501</td>\n",
       "      <td>1.654475</td>\n",
       "      <td>0.132255</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011285</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792728</td>\n",
       "      <td>0.253302</td>\n",
       "      <td>-1.245015</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.253576</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620969</td>\n",
       "      <td>-0.540695</td>\n",
       "      <td>-0.865163</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     F    M    O    U       age    income  membership_length  av_money_spent  \\\n",
       "0  1.0  0.0  0.0  0.0  1.193501  1.654475           0.132255       -0.763102   \n",
       "1  0.0  0.0  0.0  1.0  0.000000  0.000000          -0.011285       -0.763102   \n",
       "2  0.0  1.0  0.0  0.0  0.792728  0.253302          -1.245015       -0.763102   \n",
       "3  0.0  0.0  0.0  1.0  0.000000  0.000000          -0.253576       -0.763102   \n",
       "4  0.0  1.0  0.0  0.0  0.620969 -0.540695          -0.865163       -0.763102   \n",
       "\n",
       "   num_received  viewed/received  ...  offer_4  offer_5  offer_6  offer_7  \\\n",
       "0     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "1     -1.206595        -1.292364  ...        1        0        0        0   \n",
       "2     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "3     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "4     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "\n",
       "   offer_8  offer_9  informational  bogo  discount  label  \n",
       "0        0        0              0     1         0      3  \n",
       "1        0        0              0     0         1      2  \n",
       "2        0        1              0     0         1      2  \n",
       "3        0        0              0     1         0      2  \n",
       "4        1        0              0     1         0      1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data:\n",
    "data_dir = 'data'\n",
    "data_file = 'training_data_standardized.csv'\n",
    "training_data_df = training_helpers.load_training_data(data_dir=data_dir, data_file=data_file)\n",
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross entropy weights\n",
    "As it was seen in the feature engineering dataset, there are different amount of training samples for each class, so weighing during training is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f14043d5c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chceking the number of labels in the dataset:\n",
    "label_counts_df = training_data_df.label.value_counts().sort_index()\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "label_counts_df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy weights of the classes: [0.9069451  1.41665127 0.76273632 1.1357979 ]\n"
     ]
    }
   ],
   "source": [
    "# creating weighing factor for the cross entropy loss:\n",
    "label_counts = training_data_df.label.value_counts().sort_index().values\n",
    "\n",
    "cross_entropy_weights = label_counts.mean() / label_counts\n",
    "print('cross entropy weights of the classes: {}'.format(cross_entropy_weights))\n",
    "# putting it into a tensor on cuda:\n",
    "cross_entropy_weights = torch.as_tensor(cross_entropy_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>U</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>membership_length</th>\n",
       "      <th>av_money_spent</th>\n",
       "      <th>num_received</th>\n",
       "      <th>viewed/received</th>\n",
       "      <th>...</th>\n",
       "      <th>offer_4</th>\n",
       "      <th>offer_5</th>\n",
       "      <th>offer_6</th>\n",
       "      <th>offer_7</th>\n",
       "      <th>offer_8</th>\n",
       "      <th>offer_9</th>\n",
       "      <th>informational</th>\n",
       "      <th>bogo</th>\n",
       "      <th>discount</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.742123</td>\n",
       "      <td>-0.080854</td>\n",
       "      <td>-0.288567</td>\n",
       "      <td>0.144789</td>\n",
       "      <td>0.222408</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.198040</td>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>-0.004738</td>\n",
       "      <td>-0.004614</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.002159</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>0.053173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>-0.742123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>-0.393560</td>\n",
       "      <td>-0.137893</td>\n",
       "      <td>-0.210736</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>-0.035368</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>-0.027004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002849</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>-0.014170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>-0.080854</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>-0.013558</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>-0.003873</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.016564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>-0.288567</td>\n",
       "      <td>-0.393560</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>-0.232751</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.033187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.002587</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>-0.003552</td>\n",
       "      <td>-0.059276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.144789</td>\n",
       "      <td>-0.137893</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300572</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.080883</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>-0.003986</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>0.029149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0.222408</td>\n",
       "      <td>-0.210736</td>\n",
       "      <td>-0.013558</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.300572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047885</td>\n",
       "      <td>0.244121</td>\n",
       "      <td>-0.006509</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.005584</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>-0.002103</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>-0.005300</td>\n",
       "      <td>0.061189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>membership_length</th>\n",
       "      <td>0.011123</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.047885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176122</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.005336</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>-0.005100</td>\n",
       "      <td>0.070948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>av_money_spent</th>\n",
       "      <td>0.198040</td>\n",
       "      <td>-0.035368</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>-0.232751</td>\n",
       "      <td>0.080883</td>\n",
       "      <td>0.244121</td>\n",
       "      <td>0.176122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>0.396038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019673</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.029876</td>\n",
       "      <td>0.014563</td>\n",
       "      <td>-0.041691</td>\n",
       "      <td>0.077541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_received</th>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>-0.006509</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.516739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108061</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>-0.105950</td>\n",
       "      <td>0.067890</td>\n",
       "      <td>0.069505</td>\n",
       "      <td>-0.026006</td>\n",
       "      <td>0.107171</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>-0.150555</td>\n",
       "      <td>-0.023747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viewed/received</th>\n",
       "      <td>0.001956</td>\n",
       "      <td>-0.027004</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.033187</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.396038</td>\n",
       "      <td>0.516739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035243</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>-0.042932</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>-0.005509</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>-0.048585</td>\n",
       "      <td>0.018606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed/viewed</th>\n",
       "      <td>0.112884</td>\n",
       "      <td>0.002391</td>\n",
       "      <td>0.013580</td>\n",
       "      <td>-0.166805</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.096629</td>\n",
       "      <td>0.144729</td>\n",
       "      <td>0.530017</td>\n",
       "      <td>0.337040</td>\n",
       "      <td>0.414042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026592</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>-0.031542</td>\n",
       "      <td>0.020129</td>\n",
       "      <td>0.015268</td>\n",
       "      <td>-0.010146</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>-0.044630</td>\n",
       "      <td>0.058924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed_not_viewed</th>\n",
       "      <td>0.126325</td>\n",
       "      <td>-0.025500</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.120203</td>\n",
       "      <td>0.062968</td>\n",
       "      <td>0.422587</td>\n",
       "      <td>0.198433</td>\n",
       "      <td>0.103660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019720</td>\n",
       "      <td>-0.003682</td>\n",
       "      <td>-0.021145</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.018375</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>-0.024697</td>\n",
       "      <td>0.035829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_received_this</th>\n",
       "      <td>-0.003981</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.097507</td>\n",
       "      <td>0.331111</td>\n",
       "      <td>0.176541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036359</td>\n",
       "      <td>-0.008941</td>\n",
       "      <td>-0.037468</td>\n",
       "      <td>0.025908</td>\n",
       "      <td>0.026121</td>\n",
       "      <td>-0.005022</td>\n",
       "      <td>0.037621</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>-0.049828</td>\n",
       "      <td>-0.007669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viewed/received_this</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.009559</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.113534</td>\n",
       "      <td>0.283436</td>\n",
       "      <td>0.251604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071535</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>-0.015218</td>\n",
       "      <td>0.039592</td>\n",
       "      <td>0.055319</td>\n",
       "      <td>-0.037366</td>\n",
       "      <td>0.010424</td>\n",
       "      <td>0.048791</td>\n",
       "      <td>-0.059975</td>\n",
       "      <td>0.043009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed/viewed_this</th>\n",
       "      <td>0.037950</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>-0.057935</td>\n",
       "      <td>0.019797</td>\n",
       "      <td>0.035328</td>\n",
       "      <td>0.040501</td>\n",
       "      <td>0.171388</td>\n",
       "      <td>0.134594</td>\n",
       "      <td>0.138102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024854</td>\n",
       "      <td>0.053831</td>\n",
       "      <td>0.030239</td>\n",
       "      <td>-0.069729</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>-0.105789</td>\n",
       "      <td>0.056776</td>\n",
       "      <td>0.035021</td>\n",
       "      <td>0.069398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed_not_viewed_this</th>\n",
       "      <td>0.037784</td>\n",
       "      <td>-0.005542</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>-0.044390</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.031224</td>\n",
       "      <td>0.019331</td>\n",
       "      <td>0.126682</td>\n",
       "      <td>0.097160</td>\n",
       "      <td>0.036770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>-0.016795</td>\n",
       "      <td>-0.046930</td>\n",
       "      <td>0.026260</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>-0.071200</td>\n",
       "      <td>0.044032</td>\n",
       "      <td>0.017521</td>\n",
       "      <td>0.023979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_0</th>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>-0.002198</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>-0.010350</td>\n",
       "      <td>-0.034567</td>\n",
       "      <td>-0.011767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094740</td>\n",
       "      <td>-0.106043</td>\n",
       "      <td>-0.092320</td>\n",
       "      <td>-0.118282</td>\n",
       "      <td>-0.117219</td>\n",
       "      <td>-0.106820</td>\n",
       "      <td>-0.179451</td>\n",
       "      <td>0.380995</td>\n",
       "      <td>-0.236474</td>\n",
       "      <td>0.073736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_1</th>\n",
       "      <td>0.002295</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>-0.008012</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>-0.002082</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.013244</td>\n",
       "      <td>0.073504</td>\n",
       "      <td>0.021825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104960</td>\n",
       "      <td>-0.117482</td>\n",
       "      <td>-0.102279</td>\n",
       "      <td>-0.131042</td>\n",
       "      <td>-0.129864</td>\n",
       "      <td>-0.118344</td>\n",
       "      <td>-0.198810</td>\n",
       "      <td>0.422095</td>\n",
       "      <td>-0.261984</td>\n",
       "      <td>0.072617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_2</th>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.073351</td>\n",
       "      <td>0.022699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105388</td>\n",
       "      <td>-0.117960</td>\n",
       "      <td>-0.102695</td>\n",
       "      <td>-0.131575</td>\n",
       "      <td>-0.130393</td>\n",
       "      <td>-0.118825</td>\n",
       "      <td>0.658765</td>\n",
       "      <td>-0.310282</td>\n",
       "      <td>-0.263050</td>\n",
       "      <td>-0.220835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_3</th>\n",
       "      <td>-0.000737</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>-0.003768</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>-0.030938</td>\n",
       "      <td>-0.009611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095104</td>\n",
       "      <td>-0.106451</td>\n",
       "      <td>-0.092675</td>\n",
       "      <td>-0.118736</td>\n",
       "      <td>-0.117670</td>\n",
       "      <td>-0.107231</td>\n",
       "      <td>-0.180141</td>\n",
       "      <td>0.382460</td>\n",
       "      <td>-0.237383</td>\n",
       "      <td>-0.051766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_4</th>\n",
       "      <td>0.006749</td>\n",
       "      <td>-0.002849</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.019673</td>\n",
       "      <td>-0.108061</td>\n",
       "      <td>-0.035243</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094535</td>\n",
       "      <td>-0.082302</td>\n",
       "      <td>-0.105446</td>\n",
       "      <td>-0.104499</td>\n",
       "      <td>-0.095229</td>\n",
       "      <td>-0.159977</td>\n",
       "      <td>-0.248665</td>\n",
       "      <td>0.400637</td>\n",
       "      <td>-0.116674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_5</th>\n",
       "      <td>-0.004738</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.092121</td>\n",
       "      <td>-0.118026</td>\n",
       "      <td>-0.116966</td>\n",
       "      <td>-0.106590</td>\n",
       "      <td>-0.179063</td>\n",
       "      <td>-0.278331</td>\n",
       "      <td>0.448433</td>\n",
       "      <td>0.140002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_6</th>\n",
       "      <td>-0.004614</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.005584</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>-0.105950</td>\n",
       "      <td>-0.042932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082302</td>\n",
       "      <td>-0.092121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.102753</td>\n",
       "      <td>-0.101829</td>\n",
       "      <td>-0.092796</td>\n",
       "      <td>-0.155891</td>\n",
       "      <td>-0.242313</td>\n",
       "      <td>0.390403</td>\n",
       "      <td>0.148012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_7</th>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.067890</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105446</td>\n",
       "      <td>-0.118026</td>\n",
       "      <td>-0.102753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.130465</td>\n",
       "      <td>-0.118891</td>\n",
       "      <td>0.659131</td>\n",
       "      <td>-0.310455</td>\n",
       "      <td>-0.263196</td>\n",
       "      <td>-0.069124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_8</th>\n",
       "      <td>-0.002159</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>-0.005336</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.069505</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104499</td>\n",
       "      <td>-0.116966</td>\n",
       "      <td>-0.101829</td>\n",
       "      <td>-0.130465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.117823</td>\n",
       "      <td>-0.197935</td>\n",
       "      <td>0.420239</td>\n",
       "      <td>-0.260832</td>\n",
       "      <td>0.104721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_9</th>\n",
       "      <td>-0.002574</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>-0.002103</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>-0.026006</td>\n",
       "      <td>-0.005509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095229</td>\n",
       "      <td>-0.106590</td>\n",
       "      <td>-0.092796</td>\n",
       "      <td>-0.118891</td>\n",
       "      <td>-0.117823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.180376</td>\n",
       "      <td>-0.280372</td>\n",
       "      <td>0.451722</td>\n",
       "      <td>-0.063379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informational</th>\n",
       "      <td>0.004166</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>-0.002587</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.029876</td>\n",
       "      <td>0.107171</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159977</td>\n",
       "      <td>-0.179063</td>\n",
       "      <td>-0.155891</td>\n",
       "      <td>0.659131</td>\n",
       "      <td>-0.197935</td>\n",
       "      <td>-0.180376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.471006</td>\n",
       "      <td>-0.399308</td>\n",
       "      <td>-0.219992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bogo</th>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>-0.003873</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>-0.003986</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.014563</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248665</td>\n",
       "      <td>-0.278331</td>\n",
       "      <td>-0.242313</td>\n",
       "      <td>-0.310455</td>\n",
       "      <td>0.420239</td>\n",
       "      <td>-0.280372</td>\n",
       "      <td>-0.471006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.620675</td>\n",
       "      <td>0.127475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>-0.003275</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>-0.003552</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>-0.005300</td>\n",
       "      <td>-0.005100</td>\n",
       "      <td>-0.041691</td>\n",
       "      <td>-0.150555</td>\n",
       "      <td>-0.048585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400637</td>\n",
       "      <td>0.448433</td>\n",
       "      <td>0.390403</td>\n",
       "      <td>-0.263196</td>\n",
       "      <td>-0.260832</td>\n",
       "      <td>0.451722</td>\n",
       "      <td>-0.399308</td>\n",
       "      <td>-0.620675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0.053173</td>\n",
       "      <td>-0.014170</td>\n",
       "      <td>0.016564</td>\n",
       "      <td>-0.059276</td>\n",
       "      <td>0.029149</td>\n",
       "      <td>0.061189</td>\n",
       "      <td>0.070948</td>\n",
       "      <td>0.077541</td>\n",
       "      <td>-0.023747</td>\n",
       "      <td>0.018606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116674</td>\n",
       "      <td>0.140002</td>\n",
       "      <td>0.148012</td>\n",
       "      <td>-0.069124</td>\n",
       "      <td>0.104721</td>\n",
       "      <td>-0.063379</td>\n",
       "      <td>-0.219992</td>\n",
       "      <td>0.127475</td>\n",
       "      <td>0.063048</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  F         M         O         U       age  \\\n",
       "F                          1.000000 -0.742123 -0.080854 -0.288567  0.144789   \n",
       "M                         -0.742123  1.000000 -0.110272 -0.393560 -0.137893   \n",
       "O                         -0.080854 -0.110272  1.000000 -0.042878 -0.003326   \n",
       "U                         -0.288567 -0.393560 -0.042878  1.000000  0.000406   \n",
       "age                        0.144789 -0.137893 -0.003326  0.000406  1.000000   \n",
       "income                     0.222408 -0.210736 -0.013558  0.001730  0.300572   \n",
       "membership_length          0.011123 -0.003784 -0.009273 -0.007125  0.020466   \n",
       "av_money_spent             0.198040 -0.035368  0.018863 -0.232751  0.080883   \n",
       "num_received              -0.004785  0.002142 -0.004134  0.004901 -0.001565   \n",
       "viewed/received            0.001956 -0.027004  0.012073  0.033187  0.014495   \n",
       "completed/viewed           0.112884  0.002391  0.013580 -0.166805  0.046993   \n",
       "completed_not_viewed       0.126325 -0.025500  0.008031 -0.142857  0.044466   \n",
       "num_received_this         -0.003981 -0.000959  0.000132  0.006976  0.005968   \n",
       "viewed/received_this      -0.000007 -0.009559  0.002939  0.013158  0.005989   \n",
       "completed/viewed_this      0.037950  0.000828  0.010274 -0.057935  0.019797   \n",
       "completed_not_viewed_this  0.037784 -0.005542 -0.002022 -0.044390  0.015882   \n",
       "offer_0                   -0.000108 -0.001358  0.002588  0.001328 -0.002198   \n",
       "offer_1                    0.002295 -0.001056 -0.008012  0.000882 -0.002082   \n",
       "offer_2                    0.001120  0.000916  0.000654 -0.003135  0.004210   \n",
       "offer_3                   -0.000737 -0.000871  0.001314  0.001902 -0.002300   \n",
       "offer_4                    0.006749 -0.002849  0.002979 -0.006255  0.002606   \n",
       "offer_5                   -0.004738  0.003803  0.002733  0.000195 -0.000767   \n",
       "offer_6                   -0.004614  0.005320 -0.000603 -0.001151 -0.000065   \n",
       "offer_7                    0.004370 -0.004631  0.002963 -0.000275  0.005979   \n",
       "offer_8                   -0.002159 -0.000889 -0.001584  0.004855  0.000075   \n",
       "offer_9                   -0.002574  0.002432 -0.002334  0.000786 -0.005924   \n",
       "informational              0.004166 -0.002820  0.002745 -0.002587  0.007731   \n",
       "bogo                      -0.000412 -0.002588 -0.003873  0.005630 -0.003986   \n",
       "discount                  -0.003275  0.005196  0.001585 -0.003552 -0.002729   \n",
       "label                      0.053173 -0.014170  0.016564 -0.059276  0.029149   \n",
       "\n",
       "                             income  membership_length  av_money_spent  \\\n",
       "F                          0.222408           0.011123        0.198040   \n",
       "M                         -0.210736          -0.003784       -0.035368   \n",
       "O                         -0.013558          -0.009273        0.018863   \n",
       "U                          0.001730          -0.007125       -0.232751   \n",
       "age                        0.300572           0.020466        0.080883   \n",
       "income                     1.000000           0.047885        0.244121   \n",
       "membership_length          0.047885           1.000000        0.176122   \n",
       "av_money_spent             0.244121           0.176122        1.000000   \n",
       "num_received              -0.006509           0.000882        0.313841   \n",
       "viewed/received            0.028156           0.006887        0.396038   \n",
       "completed/viewed           0.096629           0.144729        0.530017   \n",
       "completed_not_viewed       0.120203           0.062968        0.422587   \n",
       "num_received_this         -0.000113          -0.000080        0.097507   \n",
       "viewed/received_this       0.006739           0.002575        0.113534   \n",
       "completed/viewed_this      0.035328           0.040501        0.171388   \n",
       "completed_not_viewed_this  0.031224           0.019331        0.126682   \n",
       "offer_0                    0.001645           0.000790       -0.010350   \n",
       "offer_1                    0.003162           0.005271        0.013244   \n",
       "offer_2                    0.000764           0.000403        0.018239   \n",
       "offer_3                   -0.003768           0.004939        0.002616   \n",
       "offer_4                   -0.001267           0.000589       -0.019673   \n",
       "offer_5                   -0.000318          -0.010036       -0.011763   \n",
       "offer_6                   -0.005584           0.001721       -0.036415   \n",
       "offer_7                    0.003583           0.001926        0.021134   \n",
       "offer_8                    0.002338          -0.005336        0.016454   \n",
       "offer_9                   -0.002103          -0.000274       -0.005191   \n",
       "informational              0.003298           0.001767        0.029876   \n",
       "bogo                       0.002279           0.003396        0.014563   \n",
       "discount                  -0.005300          -0.005100       -0.041691   \n",
       "label                      0.061189           0.070948        0.077541   \n",
       "\n",
       "                           num_received  viewed/received  ...   offer_4  \\\n",
       "F                             -0.004785         0.001956  ...  0.006749   \n",
       "M                              0.002142        -0.027004  ... -0.002849   \n",
       "O                             -0.004134         0.012073  ...  0.002979   \n",
       "U                              0.004901         0.033187  ... -0.006255   \n",
       "age                           -0.001565         0.014495  ...  0.002606   \n",
       "income                        -0.006509         0.028156  ... -0.001267   \n",
       "membership_length              0.000882         0.006887  ...  0.000589   \n",
       "av_money_spent                 0.313841         0.396038  ... -0.019673   \n",
       "num_received                   1.000000         0.516739  ... -0.108061   \n",
       "viewed/received                0.516739         1.000000  ... -0.035243   \n",
       "completed/viewed               0.337040         0.414042  ... -0.026592   \n",
       "completed_not_viewed           0.198433         0.103660  ... -0.019720   \n",
       "num_received_this              0.331111         0.176541  ... -0.036359   \n",
       "viewed/received_this           0.283436         0.251604  ... -0.071535   \n",
       "completed/viewed_this          0.134594         0.138102  ... -0.024854   \n",
       "completed_not_viewed_this      0.097160         0.036770  ...  0.003839   \n",
       "offer_0                       -0.034567        -0.011767  ... -0.094740   \n",
       "offer_1                        0.073504         0.021825  ... -0.104960   \n",
       "offer_2                        0.073351         0.022699  ... -0.105388   \n",
       "offer_3                       -0.030938        -0.009611  ... -0.095104   \n",
       "offer_4                       -0.108061        -0.035243  ...  1.000000   \n",
       "offer_5                       -0.024229        -0.002591  ... -0.094535   \n",
       "offer_6                       -0.105950        -0.042932  ... -0.082302   \n",
       "offer_7                        0.067890         0.019774  ... -0.105446   \n",
       "offer_8                        0.069505         0.027679  ... -0.104499   \n",
       "offer_9                       -0.026006        -0.005509  ... -0.095229   \n",
       "informational                  0.107171         0.032227  ... -0.159977   \n",
       "bogo                           0.053206         0.019186  ... -0.248665   \n",
       "discount                      -0.150555        -0.048585  ...  0.400637   \n",
       "label                         -0.023747         0.018606  ... -0.116674   \n",
       "\n",
       "                            offer_5   offer_6   offer_7   offer_8   offer_9  \\\n",
       "F                         -0.004738 -0.004614  0.004370 -0.002159 -0.002574   \n",
       "M                          0.003803  0.005320 -0.004631 -0.000889  0.002432   \n",
       "O                          0.002733 -0.000603  0.002963 -0.001584 -0.002334   \n",
       "U                          0.000195 -0.001151 -0.000275  0.004855  0.000786   \n",
       "age                       -0.000767 -0.000065  0.005979  0.000075 -0.005924   \n",
       "income                    -0.000318 -0.005584  0.003583  0.002338 -0.002103   \n",
       "membership_length         -0.010036  0.001721  0.001926 -0.005336 -0.000274   \n",
       "av_money_spent            -0.011763 -0.036415  0.021134  0.016454 -0.005191   \n",
       "num_received              -0.024229 -0.105950  0.067890  0.069505 -0.026006   \n",
       "viewed/received           -0.002591 -0.042932  0.019774  0.027679 -0.005509   \n",
       "completed/viewed          -0.009568 -0.031542  0.020129  0.015268 -0.010146   \n",
       "completed_not_viewed      -0.003682 -0.021145  0.011961  0.012478  0.000584   \n",
       "num_received_this         -0.008941 -0.037468  0.025908  0.026121 -0.005022   \n",
       "viewed/received_this       0.019414 -0.015218  0.039592  0.055319 -0.037366   \n",
       "completed/viewed_this      0.053831  0.030239 -0.069729  0.052714 -0.001545   \n",
       "completed_not_viewed_this  0.002604 -0.016795 -0.046930  0.026260  0.036958   \n",
       "offer_0                   -0.106043 -0.092320 -0.118282 -0.117219 -0.106820   \n",
       "offer_1                   -0.117482 -0.102279 -0.131042 -0.129864 -0.118344   \n",
       "offer_2                   -0.117960 -0.102695 -0.131575 -0.130393 -0.118825   \n",
       "offer_3                   -0.106451 -0.092675 -0.118736 -0.117670 -0.107231   \n",
       "offer_4                   -0.094535 -0.082302 -0.105446 -0.104499 -0.095229   \n",
       "offer_5                    1.000000 -0.092121 -0.118026 -0.116966 -0.106590   \n",
       "offer_6                   -0.092121  1.000000 -0.102753 -0.101829 -0.092796   \n",
       "offer_7                   -0.118026 -0.102753  1.000000 -0.130465 -0.118891   \n",
       "offer_8                   -0.116966 -0.101829 -0.130465  1.000000 -0.117823   \n",
       "offer_9                   -0.106590 -0.092796 -0.118891 -0.117823  1.000000   \n",
       "informational             -0.179063 -0.155891  0.659131 -0.197935 -0.180376   \n",
       "bogo                      -0.278331 -0.242313 -0.310455  0.420239 -0.280372   \n",
       "discount                   0.448433  0.390403 -0.263196 -0.260832  0.451722   \n",
       "label                      0.140002  0.148012 -0.069124  0.104721 -0.063379   \n",
       "\n",
       "                           informational      bogo  discount     label  \n",
       "F                               0.004166 -0.000412 -0.003275  0.053173  \n",
       "M                              -0.002820 -0.002588  0.005196 -0.014170  \n",
       "O                               0.002745 -0.003873  0.001585  0.016564  \n",
       "U                              -0.002587  0.005630 -0.003552 -0.059276  \n",
       "age                             0.007731 -0.003986 -0.002729  0.029149  \n",
       "income                          0.003298  0.002279 -0.005300  0.061189  \n",
       "membership_length               0.001767  0.003396 -0.005100  0.070948  \n",
       "av_money_spent                  0.029876  0.014563 -0.041691  0.077541  \n",
       "num_received                    0.107171  0.053206 -0.150555 -0.023747  \n",
       "viewed/received                 0.032227  0.019186 -0.048585  0.018606  \n",
       "completed/viewed                0.029175  0.017991 -0.044630  0.058924  \n",
       "completed_not_viewed            0.018375  0.008048 -0.024697  0.035829  \n",
       "num_received_this               0.037621  0.015769 -0.049828 -0.007669  \n",
       "viewed/received_this            0.010424  0.048791 -0.059975  0.043009  \n",
       "completed/viewed_this          -0.105789  0.056776  0.035021  0.069398  \n",
       "completed_not_viewed_this      -0.071200  0.044032  0.017521  0.023979  \n",
       "offer_0                        -0.179451  0.380995 -0.236474  0.073736  \n",
       "offer_1                        -0.198810  0.422095 -0.261984  0.072617  \n",
       "offer_2                         0.658765 -0.310282 -0.263050 -0.220835  \n",
       "offer_3                        -0.180141  0.382460 -0.237383 -0.051766  \n",
       "offer_4                        -0.159977 -0.248665  0.400637 -0.116674  \n",
       "offer_5                        -0.179063 -0.278331  0.448433  0.140002  \n",
       "offer_6                        -0.155891 -0.242313  0.390403  0.148012  \n",
       "offer_7                         0.659131 -0.310455 -0.263196 -0.069124  \n",
       "offer_8                        -0.197935  0.420239 -0.260832  0.104721  \n",
       "offer_9                        -0.180376 -0.280372  0.451722 -0.063379  \n",
       "informational                   1.000000 -0.471006 -0.399308 -0.219992  \n",
       "bogo                           -0.471006  1.000000 -0.620675  0.127475  \n",
       "discount                       -0.399308 -0.620675  1.000000  0.063048  \n",
       "label                          -0.219992  0.127475  0.063048  1.000000  \n",
       "\n",
       "[30 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the correlation matrix:\n",
    "training_data_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features does not seem to be correlated to eachother"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the training features and the labels\n",
    "\n",
    "It is possible to use all the data features for inference, however it is possible that some features are not that meaningful as others. For this in the following cells two kinds of methods is implemented. In the first one one is able to load the total feature vector. In the second one only some of the features are loaded, the ones which were chosen by column name in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has to be run to generate the training set if all of the features would like to be used for training.\n",
    "X = training_data_df.values[:, :-1].astype(np.float32)\n",
    "y = training_data_df.values[:, -1].astype(np.float32)\n",
    "\n",
    "# deleting the original dataframe:\n",
    "#training_data_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F', 'M', 'O', 'U', 'age', 'income', 'membership_length',\n",
       "       'av_money_spent', 'num_received', 'viewed/received', 'completed/viewed',\n",
       "       'completed_not_viewed', 'num_received_this', 'viewed/received_this',\n",
       "       'completed/viewed_this', 'completed_not_viewed_this', 'offer_0',\n",
       "       'offer_1', 'offer_2', 'offer_3', 'offer_4', 'offer_5', 'offer_6',\n",
       "       'offer_7', 'offer_8', 'offer_9', 'informational', 'bogo', 'discount',\n",
       "       'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By generating the training data with this cell, only some training features are going to be used.\n",
    "# selecting only some features:\n",
    "columns = ['F', 'M', 'O', 'U', 'age', 'income', 'membership_length',\n",
    "           'av_money_spent', 'num_received', 'viewed/received', 'completed/viewed', \n",
    "           'num_received_this', 'viewed/received_this', 'completed/viewed_this', 'completed_not_viewed_this',\n",
    "           'offer_0', 'offer_1', 'offer_2', 'offer_3', 'offer_4', 'offer_5', 'offer_6',\n",
    "           'offer_7', 'offer_8', 'offer_9']\n",
    "X = training_data_df.loc[:, columns].values.astype(np.float32)\n",
    "y = training_data_df.label.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffleing and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (49671, 25)\n",
      "validation size: (5520, 25)\n",
      "test size: (6133, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0, shuffle=True)\n",
    "print('training size: {}'.format(X_train.shape))\n",
    "print('validation size: {}'.format(X_val.shape))\n",
    "print('test size: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the acquirable accuracy with kNN\n",
    "\n",
    "Our benchmark model is a kNN classifier (k Nearest Neighbours). We are going to use sklearn's KNeighborsClassifier for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN validation accuracy: 0.463768115942029\n"
     ]
    }
   ],
   "source": [
    "model_kNN = KNeighborsClassifier(n_neighbors=10)\n",
    "model_kNN.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model_kNN.predict(X_val)\n",
    "\n",
    "accuracy_val_kNN = np.mean(y_val_pred == y_val)\n",
    "print('kNN validation accuracy: {}'.format(accuracy_val_kNN))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a training a feed forward neural network.\n",
    "\n",
    "The network implementation can be found in `source/model.py`.\n",
    "\n",
    "The solver implementation can be founf in `source.solver.py`\n",
    "\n",
    "* First the data loaders are created.\n",
    "* we check the implementation by overfitting to a single data instance\n",
    "* train the feed forward model for 4 class classification\n",
    "* train the feed forward model for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data into torch.tensor:\n",
    "X_train = torch.as_tensor(X_train, dtype=torch.float).to(device)\n",
    "X_val = torch.as_tensor(X_val, dtype=torch.float).to(device)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float).to(device)\n",
    "\n",
    "y_train = torch.as_tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val = torch.as_tensor(y_val, dtype=torch.long).to(device)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating datasets for the dataloaders:\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 512\n",
    "# creating the data loaders:\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting to a single data instance\n",
    "First we are checking the implementation by overfitting ot a single data instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data loaders for a single training insctance:\n",
    "X_train_single = torch.as_tensor(X_train[0, :].reshape(1, -1), dtype=torch.float).to(device)\n",
    "X_val_single = torch.as_tensor(X_val[0, :].reshape(1, -1), dtype=torch.float).to(device)\n",
    "\n",
    "y_train_single = torch.as_tensor(y_train[0].reshape(1), dtype=torch.long).to(device)\n",
    "y_val_single = torch.as_tensor(y_val[0].reshape(1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters:\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 4\n",
    "hidden_dims = [64]\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING\n",
      "Epoch 0/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "Iteration 0/1 train accuracy: 0.0, train_loss: 1.3149421215057373\n",
      "EPOCH 0/29 TRAIN loss/acc : 1.315/0.00%\n",
      "EPOCH 0/29 VAL loss/acc : 1.584/0.00%\n",
      "----------\n",
      "Epoch 1/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 1/29 TRAIN loss/acc : 1.189/100.00%\n",
      "EPOCH 1/29 VAL loss/acc : 1.584/0.00%\n",
      "----------\n",
      "Epoch 2/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 2/29 TRAIN loss/acc : 1.067/100.00%\n",
      "EPOCH 2/29 VAL loss/acc : 1.585/0.00%\n",
      "----------\n",
      "Epoch 3/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 3/29 TRAIN loss/acc : 0.965/100.00%\n",
      "EPOCH 3/29 VAL loss/acc : 1.586/0.00%\n",
      "----------\n",
      "Epoch 4/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 4/29 TRAIN loss/acc : 0.889/100.00%\n",
      "EPOCH 4/29 VAL loss/acc : 1.587/0.00%\n",
      "----------\n",
      "Epoch 5/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 5/29 TRAIN loss/acc : 0.836/100.00%\n",
      "EPOCH 5/29 VAL loss/acc : 1.589/0.00%\n",
      "----------\n",
      "Epoch 6/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 6/29 TRAIN loss/acc : 0.803/100.00%\n",
      "EPOCH 6/29 VAL loss/acc : 1.590/0.00%\n",
      "----------\n",
      "Epoch 7/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 7/29 TRAIN loss/acc : 0.782/100.00%\n",
      "EPOCH 7/29 VAL loss/acc : 1.591/0.00%\n",
      "----------\n",
      "Epoch 8/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 8/29 TRAIN loss/acc : 0.769/100.00%\n",
      "EPOCH 8/29 VAL loss/acc : 1.592/0.00%\n",
      "----------\n",
      "Epoch 9/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 9/29 TRAIN loss/acc : 0.761/100.00%\n",
      "EPOCH 9/29 VAL loss/acc : 1.594/0.00%\n",
      "----------\n",
      "Epoch 10/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 10/29 TRAIN loss/acc : 0.756/100.00%\n",
      "EPOCH 10/29 VAL loss/acc : 1.595/0.00%\n",
      "----------\n",
      "Epoch 11/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 11/29 TRAIN loss/acc : 0.752/100.00%\n",
      "EPOCH 11/29 VAL loss/acc : 1.596/0.00%\n",
      "----------\n",
      "Epoch 12/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 12/29 TRAIN loss/acc : 0.750/100.00%\n",
      "EPOCH 12/29 VAL loss/acc : 1.597/0.00%\n",
      "----------\n",
      "Epoch 13/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 13/29 TRAIN loss/acc : 0.748/100.00%\n",
      "EPOCH 13/29 VAL loss/acc : 1.598/0.00%\n",
      "----------\n",
      "Epoch 14/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 14/29 TRAIN loss/acc : 0.747/100.00%\n",
      "EPOCH 14/29 VAL loss/acc : 1.599/0.00%\n",
      "----------\n",
      "Epoch 15/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 15/29 TRAIN loss/acc : 0.746/100.00%\n",
      "EPOCH 15/29 VAL loss/acc : 1.600/0.00%\n",
      "----------\n",
      "Epoch 16/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 16/29 TRAIN loss/acc : 0.746/100.00%\n",
      "EPOCH 16/29 VAL loss/acc : 1.601/0.00%\n",
      "----------\n",
      "Epoch 17/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 17/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 17/29 VAL loss/acc : 1.602/0.00%\n",
      "----------\n",
      "Epoch 18/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 18/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 18/29 VAL loss/acc : 1.603/0.00%\n",
      "----------\n",
      "Epoch 19/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 19/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 19/29 VAL loss/acc : 1.604/0.00%\n",
      "----------\n",
      "Epoch 20/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 20/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 20/29 VAL loss/acc : 1.604/0.00%\n",
      "----------\n",
      "Epoch 21/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 21/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 21/29 VAL loss/acc : 1.605/0.00%\n",
      "----------\n",
      "Epoch 22/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 22/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 22/29 VAL loss/acc : 1.606/0.00%\n",
      "----------\n",
      "Epoch 23/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 23/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 23/29 VAL loss/acc : 1.606/0.00%\n",
      "----------\n",
      "Epoch 24/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 24/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 24/29 VAL loss/acc : 1.607/0.00%\n",
      "----------\n",
      "Epoch 25/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 25/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 25/29 VAL loss/acc : 1.607/0.00%\n",
      "----------\n",
      "Epoch 26/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 26/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 26/29 VAL loss/acc : 1.608/0.00%\n",
      "----------\n",
      "Epoch 27/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 27/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 27/29 VAL loss/acc : 1.608/0.00%\n",
      "----------\n",
      "Epoch 28/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 28/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 28/29 VAL loss/acc : 1.609/0.00%\n",
      "----------\n",
      "Epoch 29/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 29/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 29/29 VAL loss/acc : 1.609/0.00%\n",
      "----------\n",
      "FINISH.\n",
      "Training complete in 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# testing the model by overfitting to a single data instance:\n",
    "train_dataset_single = TensorDataset(X_train_single, y_train_single)\n",
    "val_dataset_single = TensorDataset(X_val_single, y_val_single)\n",
    "train_loader_single = torch.utils.data.DataLoader(train_dataset_single,\n",
    "                                                batch_size=1, shuffle=False, num_workers=0)\n",
    "val_loader_single = torch.utils.data.DataLoader(val_dataset_single,\n",
    "                                              batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "test_model = Linear_NN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, dropout=dropout)\n",
    "test_model.to(device)\n",
    "NN_solver = NN_Solver(optim_args={\"lr\": 5e-3, \"weight_decay\": 0},\n",
    "                loss_func=torch.nn.CrossEntropyLoss())\n",
    "best_state_dict = NN_solver.train(test_model, train_loader_single, val_loader_single, log_nth=100, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEvCAYAAABhSUTPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3xUd53/8fdnZjIJuRAICUkgBCiFcAtQSkFFbamlpGiLurpKdd318uvWdlfd7kX38nD96frb1Z+r6261/dW1v+qqrd1V27q29H6x0lvaUgjXQksh4ZJwCwm5z3z3j5lACAkZMpOczJnX8/HII+ec73fO+XAep/Du+X7nHHPOCQAAAMMT8LoAAACAdEaYAgAASAJhCgAAIAmEKQAAgCQQpgAAAJJAmAIAAEhCyKsDFxcXuxkzZnh1eAAAgIS9/PLLR5xzJQO1eRamZsyYodraWq8ODwAAkDAze2uwNob5AAAAkkCYAgAASAJhCgAAIAmEKQAAgCQQpgAAAJJAmAIAAEgCYQoAACAJhCkAAIAkEKYAAACS4NswdfxUl3783F4dae30uhQAAOBjvg1Th0526Mv3b9UjWw97XQoAAPAx34apuWUFmj4pVxu2HvK6FAAA4GO+DVNmppoFZdq4+4ia27u9LgcAAPjUkGHKzO40s0YzqxukfZ2ZbTazTWZWa2bvTH2Zw7NmYZl6ok5P7GCoDwAAjIxE7kzdJanmPO2PS1rsnFsi6VOS/j0FdaXEkooJKh2frQ11DPUBAICRMWSYcs49I+nYedpbnXMuvponyQ3Wd7QFAqY1C8r09K4mtXdFvC4HAAD4UErmTJnZB8xsh6TfKHZ3asyoWVCmju6ont7V5HUpAADAh1ISppxzv3LOzZX0fklfG6yfmd0Qn1dV29Q0OuFm+cwiTcjN0sN8qw8AAIyAlH6bLz4kOMvMigdpv8M5t8w5t6ykpCSVhx5UKBjQ6nmlemz7YXX1REflmAAAIHMkHabM7GIzs/jyUklhSUeT3W8q1SwsU0tHj557Y0yVBQAAfCA0VAczu1vSFZKKzaxe0t9LypIk59ztkn5P0ifMrFtSu6SP9JmQPiasvLhYeeGgNtQd0uVzRueOGAAAyAxDhinn3Poh2r8h6Rspq2gE5GQFtWruZD267ZD+4f0LFQyY1yUBAACf8O0T0PurWVimI61devmt416XAgAAfCRjwtQVVZMVDgV4gCcAAEipjAlT+dkhvXt2sR7eekhjbEoXAABIYxkTpiRpzYIyNZxoV13DSa9LAQAAPpFRYeqqeaUKBkwbth70uhQAAOATGRWmJuaFtWJmEfOmAABAymRUmJJi3+rb03RKuxtbvC4FAAD4QMaFqavnl0kSd6cAAEBKZFyYKivM0SWVE7SBFx8DAIAUyLgwJUk1C8pU13BS+4+1eV0KAABIcxkZptYsiA31PczdKQAAkKSMDFMzivM0t6yAMAUAAJKWkWFKin2rr/at42pq6fS6FAAAkMYyOkw5Jz267bDXpQAAgDSWsWGqqrRAMybl8q0+AACQlIwNU2amNQvLtHH3ETW3d3tdDgAASFMZG6ak2CMSeqJOT+xgqA8AAAxPRoepxRUTVDY+h6ehAwCAYcvoMBUImNYsKNXTu5rU1tXjdTkAACANZXSYkqQ1C8vU0R3VM7uavC4FAACkoYwPU8tnFGlibhZDfQAAYFgyPkyFggGtnl+qx7c3qqsn6nU5AAAgzWR8mJJiD/Bs6ezRxj1HvC4FAACkGcKUpHfMKlZ+doh39QEAgAtGmJKUkxXUqrmT9cjWw4pEndflAACANEKYiluzoFRHT3Wpdu8xr0sBAABphDAVd0XVZIVDAd7VBwAALghhKi4/O6R3zy7Ww3WH5BxDfQAAIDGEqT7WLCjTgeYObWlo9roUAACQJghTfVw1r1TBgPEATwAAkDDCVB8T88J620VFPCIBAAAkjDDVT82CMu1pOqXdjS1elwIAANIAYaqfqxeUSRJDfQAAICGEqX5Kx+doaeUEHpEAAAASMmSYMrM7zazRzOoGaf+YmW2O/2w0s8WpL3N01SwsU13DSe0/1uZ1KQAAYIxL5M7UXZJqztP+pqTLnXOLJH1N0h0pqMtTa+JDfUxEBwAAQxkyTDnnnpE06DtWnHMbnXPH46vPS6pIUW2emT4pT/PKxxOmAADAkFI9Z+rTkh5K8T49UbOgTLVvHVdjS4fXpQAAgDEsZWHKzFYpFqa+eJ4+N5hZrZnVNjU1perQI6JmYZmckx7ddtjrUgAAwBiWkjBlZosk/bukdc65o4P1c87d4Zxb5pxbVlJSkopDj5g5pfmaWZzHIxIAAMB5JR2mzKxS0i8l/YFzblfyJY0NZqY1C8r03J6jam7r9rocAAAwRiXyaIS7JT0nqcrM6s3s02Z2o5ndGO/yZUmTJH3fzDaZWe0I1juqahaWqSfq9PgOhvoAAMDAQkN1cM6tH6L9M5I+k7KKxpBFUwtVXpijh+oO6YNL0/5LigAAYATwBPTzCARiQ33P7GrSqc4er8sBAABjEGFqCGury9XZE9XjOxq9LgUAAIxBhKkhXDp9okoKsvXg5oNelwIAAMYgwtQQggHTNQvL9OTORob6AADAOQhTCegd6ntyJ0N9AADgbISpBFw2o0jF+dl6cAtDfQAA4GyEqQT0DvU9saNRbV0M9QEAgDMIUwm6prpMHd1RPbVzbL9TEAAAjC7CVIJWzJyk4vywfsNQHwAA6IMwlaBg/AGeT2xvVHtXxOtyAADAGEGYugDvrS5Xe3dET/GtPgAAEEeYugDLZxapKI+hPgAAcAZh6gKEgoHYUN+ORnV0M9QHAAAIUxfsvdXlauuK8K0+AAAgiTB1wd52UZEm5mbxAE8AACCJMHXBeof6Ht9+mKE+AABAmBqOtdXlOtUV0TO7GOoDACDTEaaG4e2zJmkCQ30AAECEqWHJCga0Zn6ZHtvOt/oAAMh0hKlhuqa6TK2dPfrt60e8LgUAAHiIMDVMKy8uVuG4LD3EUB8AABmNMDVMWcGArp5fqke3HVZnD0N9AABkKsJUEtYuKldLZ4+eZagPAICMRZhKwspZxRqfE+JdfQAAZDDCVBLCoYBWzy/To9sOq6sn6nU5AADAA4SpJL13UZlaOnr0u90M9QEAkIkIU0laeXGxChjqAwAgYxGmkpQdCmr1vFI9svUQQ30AAGQgwlQKrK0u18mOHv1uD0N9AABkGsJUCrxrTrEKskM8wBMAgAxEmEqB7FBQV80v1SPbDqs7wlAfAACZhDCVImury3WirVsb9xz1uhQAADCKCFMp8q7ZxcrPDunBzQz1AQCQSQhTKZKTFdR75k3Ww9sOMdQHAEAGIUylUO9Q3/NvMNQHAECmGDJMmdmdZtZoZnWDtM81s+fMrNPM/iL1JaaPy+eUKC8c1IN8qw8AgIyRyJ2puyTVnKf9mKTPSfpWKgpKZzlZQV05r1QPbz2sHob6AADICEOGKefcM4oFpsHaG51zL0nqTmVh6eq91WU6dqpLL7w56CkDAAA+MqpzpszsBjOrNbPapqam0Tz0qLmiarJyw0He1QcAQIYY1TDlnLvDObfMObespKRkNA89anKygrpy7mQ9XHeIoT4AADIA3+YbAWury3X0VJdeZKgPAADfI0yNgFVVkzUui6E+AAAyQSKPRrhb0nOSqsys3sw+bWY3mtmN8fYyM6uXdIukv4v3GT+yZY9t48Lxob6thxSJOq/LAQAAIyg0VAfn3Poh2g9JqkhZRT6xtrpcv9lyUC++eUxvnzXJ63IAAMAIYZhvhKyaW6KcrAAP8AQAwOcIUyMkNxzSqqrJeqiOoT4AAPyMMDWC1laX60hrp2r38q0+AAD8ijA1gq6cO1nZIYb6AADwM8LUCMrLDumKqhI9VHdIUYb6AADwJcLUCFtbXa7Glk7VvnXc61IAAMAIIEyNsPfMK1WYoT4AAHyLMDXC8rNDumJOiR6qO8hQHwAAPkSYGgVrq8t1+CRDfQAA+BFhahSsnl+qvHBQP39pv9elAACAFCNMjYK87JDWXTJV/735gJrbur0uBwAApBBhapRcv7xSnT1R3bepwetSAABAChGmRsnCqYWqnlqou1/cJ+eYiA4AgF8QpkbR9SsqteNQi17Zd8LrUgAAQIoQpkbRtYunKC8c1N0v7vO6FAAAkCKEqVGUnx3SdUviE9HbmYgOAIAfEKZG2fXLK9XRHdX9TEQHAMAXCFOjrLqiUAunjtfPXmAiOgAAfkCY8sD1y6drx6EWvbqfiegAAKQ7wpQHrlsyRbnhoO5+gYnoAACkO8KUB/KzQ1q3ZIp+vfmATnYwER0AgHRGmPLI+t6J6K8yER0AgHRGmPLIoooJWjh1vH7KRHQAANIaYcpD65fHnoi+iYnoAACkLcKUh65bHJ+IzhPRAQBIW4QpDxXkZOm6xVP069cOMhEdAIA0RZjy2PrllWrvjuj+TQe8LgUAAAwDYcpjiyoKtWAKT0QHACBdEaY8ZmZav7xS2w+e1Gv1zV6XAwAALhBhagxYt2SKxmXxRHQAANIRYWoM6J2I/sBrB9TCRHQAANIKYWqMuH4FE9EBAEhHhKkxYlFFoeaXMxEdAIB0Q5gaI8xM61dUatvBk9rMRHQAANLGkGHKzO40s0Yzqxuk3czsX81st5ltNrOlqS8zM5yeiM4T0QEASBuJ3Jm6S1LNedqvkTQ7/nODpNuSLyszjc/J0rWLy5mIDgBAGhkyTDnnnpF07Dxd1kn6sYt5XtIEMytPVYGZ5voV09XWxUR0AADSRSrmTE2VtL/Pen18G4ZhcUWh5jERHQCAtJGKMGUDbBswBZjZDWZWa2a1TU1NKTi0/5iZrl8+TdsOntSWBiaiAwAw1qUiTNVLmtZnvULSgGNUzrk7nHPLnHPLSkpKUnBof1p3yVTlZAWYiA4AQBpIRZh6QNIn4t/qe5ukZufcwRTsN2ONz8nStYum6P5NB9Ta2eN1OQAA4DwSeTTC3ZKek1RlZvVm9mkzu9HMbox3eVDSG5J2S/qBpJtGrNoMcv2KyvhE9AavSwEAAOcRGqqDc279EO1O0s0pqwiSpCXTJmhuWYHufnGfPrZiutflAACAQfAE9DHKzHT9ikrVNZzUFp6IDgDAmEWYGsPWLYlNRP8ZE9EBABizCFNjWOG42ET0BzY1MBEdAIAxijA1xq1fUalTXRE9wBPRAQAYkwhTY9wlfSaiAwCAsYcwNcaZmdYvr9SWhmYmogMAMAYRptLA+3ufiP4Sd6cAABhrCFNpoHBclt63aIruf5WJ6AAAjDWEqTSxfnlsIvqvX2MiOgAAYwlhKk0srZygqtIC/WjjXkWjzutyAABAHGEqTZiZblo1SzsOtejXm7k7BQDAWEGYSiPXLpqiuWUF+vaju9QdiXpdDgAAEGEqrQQCpr+qqdJbR9t0b+1+r8sBAAAiTKWdVVWTtWz6RH33sdfV3hXxuhwAADIeYSrNmJm+eM1cNbZ06kfP7fW6HAAAMh5hKg1dNqNIq6pKdNtTe9Tc3u11OQAAZDTCVJr6yzVz1dzerTue2eN1KQAAZDTCVJqaP2W8rls8RXc+u1eNLR1elwMAQMYiTKWxW1bPUXckqluf2O11KQAAZCzCVBqbUZynj1w2TT97YZ/2HW3zuhwAADISYSrNfe49sxUKmr7z2C6vSwEAICMRptJc6fgc/dE7Zuq+TQ3aceik1+UAAJBxCFM+8NnLZ6kgO6RvPbzT61IAAMg4hCkfKMzN0h9fPkuPbW/Uy28d87ocAAAyCmHKJz65coaK87P1jQ075ZzzuhwAADIGYconcsMhff49F+vFN4/p6V1NXpcDAEDGIEz5yEcuq9S0onH65oadika5OwUAwGggTPlIOBTQn6+u0raDJ/WbLQe9LgcAgIxAmPKZ6xZP0dyyAv3zIzvVHYl6XQ4AAL5HmPKZQMD0l2uqtPdom/6ztt7rcgAA8D3ClA9dOXeyLp0+Ud99fJc6uiNelwMAgK8RpnzIzPTFmrk6fLJTP9q41+tyAADwNcKUTy2fWaQrqkr0/af2qLm92+tyAADwLcKUj/3lmio1t3frB8+84XUpAAD4VkJhysxqzGynme02sy8N0D7dzB43s81m9pSZVaS+VFyoBVMKde3iKfrhs2+qqaXT63IAAPClIcOUmQUlfU/SNZLmS1pvZvP7dfuWpB875xZJ+qqkf0x1oRieW1bPUVckqlufeN3rUgAA8KVE7kwtl7TbOfeGc65L0j2S1vXrM1/S4/HlJwdoh0dmFufpI5dN089e3Kf9x9q8LgcAAN9JJExNlbS/z3p9fFtfr0n6vfjyByQVmNmk5MtDKnzuytkKmOk7j+7yuhQAAHwnkTBlA2zr/+K3v5B0uZm9KulySQ2Ses7ZkdkNZlZrZrVNTbyMd7SUFeboj1bO0K82NWjnoRavywEAwFcSCVP1kqb1Wa+QdKBvB+fcAefcB51zl0j62/i25v47cs7d4Zxb5pxbVlJSkkTZuFCfvXyW8rND+r8P7/S6FAAAfCWRMPWSpNlmNtPMwpI+KumBvh3MrNjMevf115LuTG2ZSNaE3LBuvHyWHtt+WC+/ddzrcgAA8I0hw5RzrkfSn0h6WNJ2Sfc657aa2VfN7Lp4tysk7TSzXZJKJX19hOpFEj65coaK87P1zQ075Fz/kVoAADAcoUQ6OecelPRgv21f7rP8X5L+K7WlIdVywyF97j0X68v3b9V/bz6oaxdP8bokAADSHk9AzzDrl1dqaeUEfekXm7WnqdXrcgAASHuEqQyTFQzoex9bquysoD77k5fV1nXOly4BAMAFIExloPLCcfruR5fo9cZW/c0vtzB/CgCAJBCmMtS7Zpfolqvm6L5NB/STF/Z5XQ4AAGmLMJXBbl51sVZVlehrv96m1/af8LocAADSEmEqgwUCpu98ZIlKCrJ1009f0fFTXV6XBABA2iFMZbgJuWHd9vGlamrp1J/du0nRKPOnAAC4EIQpaFHFBH352vl6ameTbn1yt9flAACQVghTkCR9bEWlPnDJVH3nsV367eu8hBoAgEQRpiBJMjN9/QMLNXtyvj5/zyYdONHudUkAAKQFwhROyw2HdNvHL1VXT1Q3/+wVdfVEvS4JAIAxjzCFs8wqydc3P7RIr+47of/z4HavywEAYMwjTOEca6vL9amVM3XXxr369WsHvC4HAIAxjTCFAf312rm6dPpEfekXm7W7kRciAwAwGMIUBpQVDOh71y9VTvyFyKc6eSEyAAADIUxhUGWFOfrX9ZdoT1Or/poXIgMAMCDCFM5r5cXFumX1HD3w2gH9x/NveV0OAABjDmEKQ7rpiot15dzJ+tp/b9Or+457XQ4AAGMKYQpDCgRM3/n9JSodn6Obf/qKjvFCZAAATiNMISGFuVm67WOX6khrl77w802K8EJkAAAkEaZwAaorCvWV6xbomV1N+rcnXve6HAAAxgTCFC7I+uXT9MGlU/Xdx1/X07t4ITIAAIQpXBAz09ffX62q0gLd8ONanpAOAMh4hClcsHHhoH76mRVaVFGoP737VX370V2KMocKAJChCFMYlkn52frJZ1bow5dW6F8ff11/cvcrau+KeF0WAACjjjCFYcsOBfXNDy3S366dp4fqDunD/2+jDja3e10WAACjijCFpJiZ/te7L9Kdf3iZ9h5p03W3/o4HewIAMgphCimxau5k/fKmdygnK6CP3PG87t/U4HVJAACMCsIUUmZOaYHuv/mdWjJtgj5/zyZ96+GdTEwHAPgeYQopVZQX1k8+vUIfvWyabn1ytz7705d1qrPH67IAABgxhCmkXDgU0D9+sFpfft98PbrtsD50+3NqOMHEdACAPxGmMCLMTJ9650z9/08uV/3xNq279Xd6+S0mpgMA/IcwhRF1+ZwS/eqmlcrPDmr9Hc/rl6/Ue10SAAApRZjCiLt4cr7uu3mlls2YqFvufU3/9NAORZiYDgDwiYTClJnVmNlOM9ttZl8aoL3SzJ40s1fNbLOZrU19qUhnE3LD+tGnlutjKyp1+9N79Mf/UatWJqYDAHxgyDBlZkFJ35N0jaT5ktab2fx+3f5O0r3OuUskfVTS91NdKNJfVjCgr3+gWl9dt0BP7mzSh27bqP3H2rwuCwCApCRyZ2q5pN3OuTecc12S7pG0rl8fJ2l8fLlQ0oHUlQi/+cTbZ+iuT16mAyfate57v9PPX9qnnkjU67IAABiWRMLUVEn7+6zXx7f19RVJHzezekkPSvrTlFQH33rX7BLdd/NKTSvK1Rd/sUVXfftp3b+pgblUAIC0k0iYsgG29f8Xb72ku5xzFZLWSvoPMztn32Z2g5nVmlltU1PThVcLX7moJF/33fQO/eATy5STFdTn79mka777jDbUHZRzhCoAQHpIJEzVS5rWZ71C5w7jfVrSvZLknHtOUo6k4v47cs7d4Zxb5pxbVlJSMryK4StmptXzS/Xg596lW6+/RD1Rpxt/8oquvfVZPbmjkVAFABjzEglTL0mabWYzzSys2ATzB/r12SfpPZJkZvMUC1PcekLCAgHT+xZN0SNfeLf++cOL1dzerU/e9ZJ+77aN2rj7iNflAQAwKEvk//zjjzr4F0lBSXc6575uZl+VVOuceyD+7b4fSMpXbAjwr5xzj5xvn8uWLXO1tbVJ/wHgT92RqP6ztl7/9sTrOtjcoXfMmqQ/v3qOLp1e5HVpAIAMZGYvO+eWDdjm1TAKYQqJ6OiO6O4X9+l7T+7RkdZOraoq0Z9fXaWFUwu9Lg0AkEEIU0h7bV09+vFzb+n2p/foRFu3ahaU6c9Wz1FVWYHXpQEAMgBhCr7R0tGtHz77pn742zfV2tWj6xZP0ReumqOZxXlelwYA8DHCFHzn+Kku3fHbN3TX7/aqKxJVzcIyvbe6XFdUlSg3HPK6PACAzxCm4FtNLZ26/ek9uu/VBh091aWcrIAun1OiaxaW68p5kzU+J8vrEgEAPkCYgu/1RKJ6ae9xbag7qA1bD+nwyU6FgwG9c3axahaWafW8Uk3MC3tdJgAgTRGmkFGiUadX95/QhrqDenDLITWcaFcwYHr7RZNUs7BMVy8o1eSCHK/LBACkEcIUMpZzTnUNJ/VQ3UFtqDukN46ckpl02fQi1SwsU83CMk2ZMM7rMgEAYxxhClAsWO063Ho6WO041CJJWjxtgq5ZWKaaBWWaPilXZgO9jhIAkMkIU8AA3jxy6nSw2lzfLEmalBfWwqmFWlRRqIVTC1U9tVDlhTkELADIcIQpYAj1x9v05I5GvVbfrLqGZr3e2KpINPbfRm/Aqp4aD1gVhZpCwAKAjHK+MMUDeQBJFRNz9Qdvn6E/iK93dEe07eBJ1TU0a0t9s7Y0NOvZ3UdOB6yi0wFrvKqnFqq6YgIBCwAyFGEKGEBOVlBLKydqaeXE09sGCli3DxCwqkrzVVmUq4qiXFUW5WrqhHHKyQp69UcBAIwwwhSQoEQD1vNvHFVXT/Ssz5aNz9G0onGaVpSraRNjIWtaPGxNLshWIMAdLQBIV4QpIAkDBaxo1KmptVP7jrVp/7G2+O927T/Wpuf2HNWvTjao71TFcCigionjToesyqJcVUwcp5KCbE3Kz9ak/LAKskMMIQLAGEWYAlIsEDCVjs9R6fgcXTaj6Jz2zp6IGo63x0LW8VjI6g1dr+47rpMdPed8JhwMqCgvrEn5YU3Kz1Zxn+VJeWEV52efbi/Oz2ZYEQBGEWEKGGXZoaAuKsnXRSX5A7Y3t3Wr/kSbjrZ26eipTh1t7dKR1i4dbe3U0VOx33saW3X0VKc6uqMD7iMvHNSk/GxNyM1SQU5IBdlZys8JxZZzsjQ+J6T87NhywentZ9bHZQW5EwYACSJMAWNMYW6WCnMLE+rb1tUTD1udp8PXkdYuHTsV29bc3q2Wjh41tbSqpaNHLR09au08985Xf8GAxcNWLGDlhoMalxVUTlZQ48JBjcsKaFxWUOPCofjvwOn23HBI48KBWN/T/WNt4WBA4VDsJxQwAhsAXyBMAWksNxxSblFI04pyE/5MNOrU2tUTD1exsNXa0aOT8eVY4Dqz3NLRo47uSCy4nepSR3dE7V0RtXfHfvpPtk+UmU6Hq+xQ4KygFT5rPRbCsvuEsFAwoKygKRSI/z5rOdYn1jegUNDO7hsIKBg0hQKmoJmCgXN/QoGAggEpGAjE+gQH6WumQEAKWGzZTAREIAMRpoAMEwiYxudkaXxOlqTk30vYE4mqoyeq9q5ILGjFw1Zbn/W2rog6e2LB6/RPJPa7s89y/7aunqia27vjyxF1RaLqiTh1R5x6or3LUfVE3elHVHgtYLE7e2ZnApjFt8UClykYD2CBeHsgHsLMerfHfluf5d4+Z9bP9O/728xk0plt6rtvyXSmv/r0CfT5rPr0i7X17jvW2Lu9/zEU32b9+kiD73ugz6i3b5+2M8t9tvcJrqePM8C+e7f3XT/zufj2s47Xr63PZ0x9Omrw45y97ez1/v0GWTyntoHrH2SfA/Q9u61f3wH79FtPcP+DHXbwegZuuJD9TyvK1bzy8YMdYMQRpgAkJRQMKD8YUH62t3+dRKNO3fGA1RM5s9wbtnoi0TMhLOoUjbqzfkecUyQS3+bcefpEFXFSJBpVJCpFXaw94pyiLlZH1MXXo7Ftkfi2qHOKRGPviYzEP+NcfB8utv3M+plt0X59YsdUfH+xdafYuuvdj3R6X72/1bdPfFl9963ez8fO6Tn7UW/bmX24vrXHmk5v7423ru++T/dxfdrOPhZwoT7+tkr9w/urPTs+YQqALwQCpuxAUB5nOqRQb7iUzgSuM8u9291ZAcz1C2q9/dXv833bNcC++n9moM85nd25bw7s3+esGvv9Gft/Rme19/2cO09b/8+dW/fgxzn3wP37DL2PgWscsn+K9jMxLzxwwyjhrx0AwJjUOzzZZ4tXpQDnFfC6AAAAgHRGmAIAAEgCYQoAACAJhCkAAIAkEKYAAACSQJgCAABIAmEKAAAgCYQpAACAJBCmAAAAkkCYAgAASII5j94qaWZNkt4ahUMVSzoyCsfJRJzbkcO5HVmc35HDuR1ZnN+RM9S5ne6cKxmowbMwNVrMrNY5txZnJ24AAAP1SURBVMzrOvyIcztyOLcji/M7cji3I4vzO3KSObcM8wEAACSBMAUAAJCETAhTd3hdgI9xbkcO53ZkcX5HDud2ZHF+R86wz63v50wBAACMpEy4MwUAADBifBumzKzGzHaa2W4z+5LX9fiNme01sy1mtsnMar2uJ52Z2Z1m1mhmdX22FZnZo2b2evz3RC9rTGeDnN+vmFlD/PrdZGZrvawxXZnZNDN70sy2m9lWM/t8fDvXb5LOc265dlPAzHLM7EUzey1+fv93fPtMM3shfu3+3MzCCe3Pj8N8ZhaUtEvSakn1kl6StN45t83TwnzEzPZKWuac43knSTKzd0tqlfRj59zC+LZvSjrmnPun+P8MTHTOfdHLOtPVIOf3K5JanXPf8rK2dGdm5ZLKnXOvmFmBpJclvV/SH4nrNynnObe/L67dpJmZScpzzrWaWZakZyV9XtItkn7pnLvHzG6X9Jpz7rah9ufXO1PLJe12zr3hnOuSdI+kdR7XBAzIOfeMpGP9Nq+T9KP48o8U+0sUwzDI+UUKOOcOOudeiS+3SNouaaq4fpN2nnOLFHAxrfHVrPiPk3SlpP+Kb0/42vVrmJoqaX+f9XpxEaaak/SImb1sZjd4XYwPlTrnDkqxv1QlTfa4Hj/6EzPbHB8GZBgqSWY2Q9Ilkl4Q129K9Tu3EtduSphZ0Mw2SWqU9KikPZJOOOd64l0Szg5+DVM2wDb/jWd6a6VzbqmkayTdHB9KAdLFbZJmSVoi6aCkf/a2nPRmZvmSfiHpC865k17X4ycDnFuu3RRxzkWcc0skVSg2ojVvoG6J7MuvYape0rQ+6xWSDnhUiy855w7EfzdK+pViFyJS53B8zkTv3IlGj+vxFefc4fhfpFFJPxDX77DF55v8QtJPnXO/jG/m+k2Bgc4t127qOedOSHpK0tskTTCzULwp4ezg1zD1kqTZ8Vn5YUkflfSAxzX5hpnlxSdEyszyJF0tqe78n8IFekDSH8aX/1DS/R7W4ju9/9DHfUBcv8MSn8T7Q0nbnXPf7tPE9Zukwc4t125qmFmJmU2IL4+TdJVi89KelPSheLeEr11ffptPkuJfF/0XSUFJdzrnvu5xSb5hZhcpdjdKkkKSfsb5HT4zu1vSFYq9sfywpL+XdJ+keyVVSton6cPOOSZRD8Mg5/cKxYZJnKS9kv64d44PEmdm75T0W0lbJEXjm/9Gsbk9XL9JOM+5XS+u3aSZ2SLFJpgHFbuxdK9z7qvxf9/ukVQk6VVJH3fOdQ65P7+GKQAAgNHg12E+AACAUUGYAgAASAJhCgAAIAmEKQAAgCQQpgAAAJJAmAIAAEgCYQoAACAJhCkAAIAk/A+cYWF7Am7mfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing the train loss history:\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(NN_solver.train_loss_history)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the whole dataset.\n",
    "\n",
    "After we have validated that the model learns, since it was able to overfit to a single data instance, we can train a more complex architecture on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# network parameters:\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 4\n",
    "hidden_dims = [256, 512, 512, 128]\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING\n",
      "Epoch 0/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 0/99 TRAIN loss/acc : 2.681/32.86%\n",
      "EPOCH 0/99 VAL loss/acc : 2.619/42.10%\n",
      "----------\n",
      "Epoch 1/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 1/99 TRAIN loss/acc : 2.578/39.77%\n",
      "EPOCH 1/99 VAL loss/acc : 2.514/47.25%\n",
      "----------\n",
      "Epoch 2/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 2/99 TRAIN loss/acc : 2.526/42.89%\n",
      "EPOCH 2/99 VAL loss/acc : 2.466/49.58%\n",
      "----------\n",
      "Epoch 3/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 3/99 TRAIN loss/acc : 2.511/44.35%\n",
      "EPOCH 3/99 VAL loss/acc : 2.452/49.53%\n",
      "----------\n",
      "Epoch 4/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 4/99 TRAIN loss/acc : 2.496/44.97%\n",
      "EPOCH 4/99 VAL loss/acc : 2.444/49.86%\n",
      "----------\n",
      "Epoch 5/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 5/99 TRAIN loss/acc : 2.494/44.95%\n",
      "EPOCH 5/99 VAL loss/acc : 2.438/49.73%\n",
      "----------\n",
      "Epoch 6/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 6/99 TRAIN loss/acc : 2.489/45.34%\n",
      "EPOCH 6/99 VAL loss/acc : 2.435/50.63%\n",
      "----------\n",
      "Epoch 7/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 7/99 TRAIN loss/acc : 2.489/45.56%\n",
      "EPOCH 7/99 VAL loss/acc : 2.435/50.91%\n",
      "----------\n",
      "Epoch 8/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 8/99 TRAIN loss/acc : 2.485/45.86%\n",
      "EPOCH 8/99 VAL loss/acc : 2.433/50.29%\n",
      "----------\n",
      "Epoch 9/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 9/99 TRAIN loss/acc : 2.479/46.10%\n",
      "EPOCH 9/99 VAL loss/acc : 2.432/50.67%\n",
      "----------\n",
      "Epoch 10/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 10/99 TRAIN loss/acc : 2.476/46.01%\n",
      "EPOCH 10/99 VAL loss/acc : 2.432/50.49%\n",
      "----------\n",
      "Epoch 11/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 11/99 TRAIN loss/acc : 2.479/46.15%\n",
      "EPOCH 11/99 VAL loss/acc : 2.429/51.07%\n",
      "----------\n",
      "Epoch 12/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 12/99 TRAIN loss/acc : 2.474/46.32%\n",
      "EPOCH 12/99 VAL loss/acc : 2.429/51.11%\n",
      "----------\n",
      "Epoch 13/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 13/99 TRAIN loss/acc : 2.477/46.11%\n",
      "EPOCH 13/99 VAL loss/acc : 2.427/50.83%\n",
      "----------\n",
      "Epoch 14/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 14/99 TRAIN loss/acc : 2.470/46.30%\n",
      "EPOCH 14/99 VAL loss/acc : 2.426/51.07%\n",
      "----------\n",
      "Epoch 15/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 15/99 TRAIN loss/acc : 2.476/46.54%\n",
      "EPOCH 15/99 VAL loss/acc : 2.426/50.58%\n",
      "----------\n",
      "Epoch 16/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 16/99 TRAIN loss/acc : 2.473/46.41%\n",
      "EPOCH 16/99 VAL loss/acc : 2.427/50.80%\n",
      "----------\n",
      "Epoch 17/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 17/99 TRAIN loss/acc : 2.471/46.48%\n",
      "EPOCH 17/99 VAL loss/acc : 2.428/50.16%\n",
      "----------\n",
      "Epoch 18/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 18/99 TRAIN loss/acc : 2.476/45.99%\n",
      "EPOCH 18/99 VAL loss/acc : 2.426/51.01%\n",
      "----------\n",
      "Epoch 19/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 19/99 TRAIN loss/acc : 2.470/46.69%\n",
      "EPOCH 19/99 VAL loss/acc : 2.426/50.96%\n",
      "----------\n",
      "Epoch 20/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 20/99 TRAIN loss/acc : 2.471/46.82%\n",
      "EPOCH 20/99 VAL loss/acc : 2.425/50.94%\n",
      "----------\n",
      "Epoch 21/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 21/99 TRAIN loss/acc : 2.473/46.70%\n",
      "EPOCH 21/99 VAL loss/acc : 2.426/50.54%\n",
      "----------\n",
      "Epoch 22/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 22/99 TRAIN loss/acc : 2.472/46.65%\n",
      "EPOCH 22/99 VAL loss/acc : 2.425/51.11%\n",
      "----------\n",
      "Epoch 23/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 23/99 TRAIN loss/acc : 2.468/46.88%\n",
      "EPOCH 23/99 VAL loss/acc : 2.423/51.43%\n",
      "----------\n",
      "Epoch 24/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 24/99 TRAIN loss/acc : 2.469/46.76%\n",
      "EPOCH 24/99 VAL loss/acc : 2.423/51.09%\n",
      "----------\n",
      "Epoch 25/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 25/99 TRAIN loss/acc : 2.464/46.84%\n",
      "EPOCH 25/99 VAL loss/acc : 2.423/50.83%\n",
      "----------\n",
      "Epoch 26/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 26/99 TRAIN loss/acc : 2.461/46.89%\n",
      "EPOCH 26/99 VAL loss/acc : 2.422/51.63%\n",
      "----------\n",
      "Epoch 27/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 27/99 TRAIN loss/acc : 2.466/47.03%\n",
      "EPOCH 27/99 VAL loss/acc : 2.423/51.38%\n",
      "----------\n",
      "Epoch 28/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 28/99 TRAIN loss/acc : 2.464/46.76%\n",
      "EPOCH 28/99 VAL loss/acc : 2.422/50.96%\n",
      "----------\n",
      "Epoch 29/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 29/99 TRAIN loss/acc : 2.467/46.67%\n",
      "EPOCH 29/99 VAL loss/acc : 2.423/51.32%\n",
      "----------\n",
      "Epoch 30/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 30/99 TRAIN loss/acc : 2.468/47.04%\n",
      "EPOCH 30/99 VAL loss/acc : 2.423/50.94%\n",
      "----------\n",
      "Epoch 31/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 31/99 TRAIN loss/acc : 2.464/46.85%\n",
      "EPOCH 31/99 VAL loss/acc : 2.423/50.94%\n",
      "----------\n",
      "Epoch 32/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 32/99 TRAIN loss/acc : 2.464/46.66%\n",
      "EPOCH 32/99 VAL loss/acc : 2.422/51.56%\n",
      "----------\n",
      "Epoch 33/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 33/99 TRAIN loss/acc : 2.456/47.00%\n",
      "EPOCH 33/99 VAL loss/acc : 2.420/51.58%\n",
      "----------\n",
      "Epoch 34/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 34/99 TRAIN loss/acc : 2.462/47.14%\n",
      "EPOCH 34/99 VAL loss/acc : 2.419/51.39%\n",
      "----------\n",
      "Epoch 35/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 35/99 TRAIN loss/acc : 2.464/46.78%\n",
      "EPOCH 35/99 VAL loss/acc : 2.421/51.45%\n",
      "----------\n",
      "Epoch 36/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 36/99 TRAIN loss/acc : 2.464/46.90%\n",
      "EPOCH 36/99 VAL loss/acc : 2.421/51.32%\n",
      "----------\n",
      "Epoch 37/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 37/99 TRAIN loss/acc : 2.461/47.00%\n",
      "EPOCH 37/99 VAL loss/acc : 2.422/51.09%\n",
      "----------\n",
      "Epoch 38/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 38/99 TRAIN loss/acc : 2.458/46.91%\n",
      "EPOCH 38/99 VAL loss/acc : 2.421/51.39%\n",
      "----------\n",
      "Epoch 39/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 39/99 TRAIN loss/acc : 2.463/46.96%\n",
      "EPOCH 39/99 VAL loss/acc : 2.421/51.58%\n",
      "----------\n",
      "Epoch 40/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 40/99 TRAIN loss/acc : 2.453/47.48%\n",
      "EPOCH 40/99 VAL loss/acc : 2.419/51.29%\n",
      "----------\n",
      "Epoch 41/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 41/99 TRAIN loss/acc : 2.463/47.16%\n",
      "EPOCH 41/99 VAL loss/acc : 2.419/51.45%\n",
      "----------\n",
      "Epoch 42/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 42/99 TRAIN loss/acc : 2.459/47.32%\n",
      "EPOCH 42/99 VAL loss/acc : 2.419/51.01%\n",
      "----------\n",
      "Epoch 43/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 43/99 TRAIN loss/acc : 2.458/46.80%\n",
      "EPOCH 43/99 VAL loss/acc : 2.421/51.41%\n",
      "----------\n",
      "Epoch 44/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 44/99 TRAIN loss/acc : 2.460/47.20%\n",
      "EPOCH 44/99 VAL loss/acc : 2.420/50.92%\n",
      "----------\n",
      "Epoch 45/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 45/99 TRAIN loss/acc : 2.461/47.21%\n",
      "EPOCH 45/99 VAL loss/acc : 2.420/51.41%\n",
      "----------\n",
      "Epoch 46/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 46/99 TRAIN loss/acc : 2.457/47.37%\n",
      "EPOCH 46/99 VAL loss/acc : 2.418/51.07%\n",
      "----------\n",
      "Epoch 47/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 47/99 TRAIN loss/acc : 2.461/47.33%\n",
      "EPOCH 47/99 VAL loss/acc : 2.419/51.30%\n",
      "----------\n",
      "Epoch 48/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 48/99 TRAIN loss/acc : 2.461/47.20%\n",
      "EPOCH 48/99 VAL loss/acc : 2.418/51.12%\n",
      "----------\n",
      "Epoch 49/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 49/99 TRAIN loss/acc : 2.462/47.07%\n",
      "EPOCH 49/99 VAL loss/acc : 2.418/51.41%\n",
      "----------\n",
      "Epoch 50/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 50/99 TRAIN loss/acc : 2.463/47.27%\n",
      "EPOCH 50/99 VAL loss/acc : 2.419/51.59%\n",
      "----------\n",
      "Epoch 51/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 51/99 TRAIN loss/acc : 2.459/47.32%\n",
      "EPOCH 51/99 VAL loss/acc : 2.419/51.67%\n",
      "----------\n",
      "Epoch 52/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 52/99 TRAIN loss/acc : 2.459/47.25%\n",
      "EPOCH 52/99 VAL loss/acc : 2.420/51.59%\n",
      "----------\n",
      "Epoch 53/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 53/99 TRAIN loss/acc : 2.460/47.44%\n",
      "EPOCH 53/99 VAL loss/acc : 2.419/51.38%\n",
      "----------\n",
      "Epoch 54/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 54/99 TRAIN loss/acc : 2.462/47.35%\n",
      "EPOCH 54/99 VAL loss/acc : 2.417/51.52%\n",
      "----------\n",
      "Epoch 55/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 55/99 TRAIN loss/acc : 2.458/47.29%\n",
      "EPOCH 55/99 VAL loss/acc : 2.417/51.74%\n",
      "----------\n",
      "Epoch 56/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 56/99 TRAIN loss/acc : 2.458/47.31%\n",
      "EPOCH 56/99 VAL loss/acc : 2.417/51.45%\n",
      "----------\n",
      "Epoch 57/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 57/99 TRAIN loss/acc : 2.447/47.44%\n",
      "EPOCH 57/99 VAL loss/acc : 2.419/51.76%\n",
      "----------\n",
      "Epoch 58/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 58/99 TRAIN loss/acc : 2.456/47.17%\n",
      "EPOCH 58/99 VAL loss/acc : 2.418/51.61%\n",
      "----------\n",
      "Epoch 59/99\n",
      "learning rate : 0.0001\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 59/99 TRAIN loss/acc : 2.459/47.01%\n",
      "EPOCH 59/99 VAL loss/acc : 2.419/51.49%\n",
      "----------\n",
      "Epoch 60/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 60/99 TRAIN loss/acc : 2.457/47.39%\n",
      "EPOCH 60/99 VAL loss/acc : 2.419/51.47%\n",
      "----------\n",
      "Epoch 61/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 61/99 TRAIN loss/acc : 2.456/47.45%\n",
      "EPOCH 61/99 VAL loss/acc : 2.419/51.47%\n",
      "----------\n",
      "Epoch 62/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 62/99 TRAIN loss/acc : 2.452/47.57%\n",
      "EPOCH 62/99 VAL loss/acc : 2.419/51.30%\n",
      "----------\n",
      "Epoch 63/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 63/99 TRAIN loss/acc : 2.453/47.38%\n",
      "EPOCH 63/99 VAL loss/acc : 2.420/51.05%\n",
      "----------\n",
      "Epoch 64/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 64/99 TRAIN loss/acc : 2.455/47.40%\n",
      "EPOCH 64/99 VAL loss/acc : 2.417/51.78%\n",
      "----------\n",
      "Epoch 65/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 65/99 TRAIN loss/acc : 2.458/47.00%\n",
      "EPOCH 65/99 VAL loss/acc : 2.416/51.59%\n",
      "----------\n",
      "Epoch 66/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 66/99 TRAIN loss/acc : 2.452/47.26%\n",
      "EPOCH 66/99 VAL loss/acc : 2.418/51.65%\n",
      "----------\n",
      "Epoch 67/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 67/99 TRAIN loss/acc : 2.456/47.21%\n",
      "EPOCH 67/99 VAL loss/acc : 2.419/51.41%\n",
      "----------\n",
      "Epoch 68/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 68/99 TRAIN loss/acc : 2.454/47.56%\n",
      "EPOCH 68/99 VAL loss/acc : 2.418/51.79%\n",
      "----------\n",
      "Epoch 69/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 69/99 TRAIN loss/acc : 2.448/47.69%\n",
      "EPOCH 69/99 VAL loss/acc : 2.417/52.07%\n",
      "----------\n",
      "Epoch 70/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 70/99 TRAIN loss/acc : 2.452/47.73%\n",
      "EPOCH 70/99 VAL loss/acc : 2.418/51.38%\n",
      "----------\n",
      "Epoch 71/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 71/99 TRAIN loss/acc : 2.453/47.63%\n",
      "EPOCH 71/99 VAL loss/acc : 2.417/51.45%\n",
      "----------\n",
      "Epoch 72/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 72/99 TRAIN loss/acc : 2.458/47.42%\n",
      "EPOCH 72/99 VAL loss/acc : 2.419/51.21%\n",
      "----------\n",
      "Epoch 73/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 73/99 TRAIN loss/acc : 2.454/47.47%\n",
      "EPOCH 73/99 VAL loss/acc : 2.418/51.50%\n",
      "----------\n",
      "Epoch 74/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 74/99 TRAIN loss/acc : 2.445/47.90%\n",
      "EPOCH 74/99 VAL loss/acc : 2.417/51.54%\n",
      "----------\n",
      "Epoch 75/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 75/99 TRAIN loss/acc : 2.453/47.44%\n",
      "EPOCH 75/99 VAL loss/acc : 2.416/51.45%\n",
      "----------\n",
      "Epoch 76/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 76/99 TRAIN loss/acc : 2.454/47.33%\n",
      "EPOCH 76/99 VAL loss/acc : 2.416/51.83%\n",
      "----------\n",
      "Epoch 77/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 77/99 TRAIN loss/acc : 2.455/47.47%\n",
      "EPOCH 77/99 VAL loss/acc : 2.417/51.68%\n",
      "----------\n",
      "Epoch 78/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 78/99 TRAIN loss/acc : 2.455/47.34%\n",
      "EPOCH 78/99 VAL loss/acc : 2.415/51.85%\n",
      "----------\n",
      "Epoch 79/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 79/99 TRAIN loss/acc : 2.452/47.70%\n",
      "EPOCH 79/99 VAL loss/acc : 2.417/51.85%\n",
      "----------\n",
      "Epoch 80/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 80/99 TRAIN loss/acc : 2.456/47.57%\n",
      "EPOCH 80/99 VAL loss/acc : 2.419/51.63%\n",
      "----------\n",
      "Epoch 81/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 81/99 TRAIN loss/acc : 2.454/47.42%\n",
      "EPOCH 81/99 VAL loss/acc : 2.418/51.54%\n",
      "----------\n",
      "Epoch 82/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 82/99 TRAIN loss/acc : 2.450/47.70%\n",
      "EPOCH 82/99 VAL loss/acc : 2.417/51.50%\n",
      "----------\n",
      "Epoch 83/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 83/99 TRAIN loss/acc : 2.450/47.77%\n",
      "EPOCH 83/99 VAL loss/acc : 2.417/51.45%\n",
      "----------\n",
      "Epoch 84/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 84/99 TRAIN loss/acc : 2.450/47.80%\n",
      "EPOCH 84/99 VAL loss/acc : 2.417/51.56%\n",
      "----------\n",
      "Epoch 85/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 85/99 TRAIN loss/acc : 2.450/47.55%\n",
      "EPOCH 85/99 VAL loss/acc : 2.417/51.85%\n",
      "----------\n",
      "Epoch 86/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 86/99 TRAIN loss/acc : 2.449/47.89%\n",
      "EPOCH 86/99 VAL loss/acc : 2.418/51.43%\n",
      "----------\n",
      "Epoch 87/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 87/99 TRAIN loss/acc : 2.453/47.80%\n",
      "EPOCH 87/99 VAL loss/acc : 2.416/51.41%\n",
      "----------\n",
      "Epoch 88/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 88/99 TRAIN loss/acc : 2.448/47.54%\n",
      "EPOCH 88/99 VAL loss/acc : 2.417/51.61%\n",
      "----------\n",
      "Epoch 89/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 89/99 TRAIN loss/acc : 2.446/47.84%\n",
      "EPOCH 89/99 VAL loss/acc : 2.418/51.54%\n",
      "----------\n",
      "Epoch 90/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 90/99 TRAIN loss/acc : 2.455/47.53%\n",
      "EPOCH 90/99 VAL loss/acc : 2.417/51.65%\n",
      "----------\n",
      "Epoch 91/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 91/99 TRAIN loss/acc : 2.458/47.67%\n",
      "EPOCH 91/99 VAL loss/acc : 2.415/51.78%\n",
      "----------\n",
      "Epoch 92/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 92/99 TRAIN loss/acc : 2.445/47.85%\n",
      "EPOCH 92/99 VAL loss/acc : 2.417/51.36%\n",
      "----------\n",
      "Epoch 93/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 93/99 TRAIN loss/acc : 2.447/47.98%\n",
      "EPOCH 93/99 VAL loss/acc : 2.417/51.36%\n",
      "----------\n",
      "Epoch 94/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 94/99 TRAIN loss/acc : 2.449/47.41%\n",
      "EPOCH 94/99 VAL loss/acc : 2.417/51.34%\n",
      "----------\n",
      "Epoch 95/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 95/99 TRAIN loss/acc : 2.455/47.55%\n",
      "EPOCH 95/99 VAL loss/acc : 2.417/51.52%\n",
      "----------\n",
      "Epoch 96/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 96/99 TRAIN loss/acc : 2.448/47.56%\n",
      "EPOCH 96/99 VAL loss/acc : 2.418/51.41%\n",
      "----------\n",
      "Epoch 97/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 97/99 TRAIN loss/acc : 2.447/47.79%\n",
      "EPOCH 97/99 VAL loss/acc : 2.417/51.47%\n",
      "----------\n",
      "Epoch 98/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 98/99 TRAIN loss/acc : 2.454/47.64%\n",
      "EPOCH 98/99 VAL loss/acc : 2.416/51.76%\n",
      "----------\n",
      "Epoch 99/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 99/99 TRAIN loss/acc : 2.450/47.59%\n",
      "EPOCH 99/99 VAL loss/acc : 2.417/51.72%\n",
      "----------\n",
      "FINISH.\n",
      "Training complete in 1m 25s\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model:\n",
    "model_NN = Linear_NN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim,\n",
    "                     dropout=dropout, activation='leaky_relu')\n",
    "model_NN.to(device)\n",
    "\n",
    "saving_path = 'models/NN_model.pth'\n",
    "\n",
    "NN_solver = NN_Solver(optim_args={\"lr\": 1e-4, \"weight_decay\": 0},\n",
    "            loss_func=torch.nn.CrossEntropyLoss(weight=cross_entropy_weights))\n",
    "NN_solver.train(model_NN, train_loader, val_loader, log_nth=0, num_epochs=100, saving_path=saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy: 0.5206521739130435\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing the train and validation loss history:\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(NN_solver.train_loss_history)\n",
    "plt.plot(NN_solver.val_loss_history)\n",
    "\n",
    "print('best validation accuracy: {}'.format(max(NN_solver.val_acc_history)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "After the hyper parameters were set with the help of the validation set, the mosel can be tested on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.5196478069460296\n"
     ]
    }
   ],
   "source": [
    "# testing the trained model:\n",
    "model_NN.eval()\n",
    "output = model_NN(X_test)\n",
    "pred_labels = output.argmax(dim=1)\n",
    "\n",
    "test_accuracy = np.mean((pred_labels == y_test).cpu().numpy())\n",
    "print('test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: tensor([2, 2, 3, 3, 3, 0, 3, 0, 3, 3, 2, 3, 1, 3, 3, 3, 2, 2, 3, 0],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "real:      tensor([1, 0, 3, 3, 3, 2, 3, 0, 3, 3, 2, 0, 3, 2, 3, 2, 2, 0, 3, 0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# some sample labels:\n",
    "print('predicted: {}'.format(pred_labels[500: 520]))\n",
    "print('real:      {}'.format(y_test[500: 520]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural net only for classifying succesful and not succesful offers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cathegorizing the offers into succesful and not succesful. Labels 0 and 1 correspond to not succesful offers and labels 2 and 3 correspond to succesful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By generating the training data with this cell, only some training features are going to be used.\n",
    "# selecting only some features:\n",
    "columns = ['F', 'M', 'O', 'U', 'age', 'income', 'membership_length',\n",
    "           'av_money_spent', 'num_received', 'viewed/received', 'completed/viewed', \n",
    "           'num_received_this', 'viewed/received_this', 'completed/viewed_this', 'completed_not_viewed_this',\n",
    "           'offer_0', 'offer_1', 'offer_2', 'offer_3', 'offer_4',\n",
    "           'offer_5', 'offer_6', 'offer_7', 'offer_8', 'offer_9']\n",
    "X = training_data_df.loc[:, columns].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of succesful points: 20964\n",
      "number of not succesful points: 40360\n"
     ]
    }
   ],
   "source": [
    "# normal successful offers:\n",
    "mask_1 = training_data_df.label > 2.5\n",
    "# informational successful offers:\n",
    "mask_2 = ((training_data_df.offer_2 == 1) | (training_data_df.offer_7 == 1)) & (training_data_df.label == 2)\n",
    "# every successful offer:\n",
    "success_df = mask_1 | mask_2\n",
    "print('number of succesful points: {}'.format(success_df.sum()))\n",
    "print('number of not succesful points: {}'.format(len(success_df.values) - success_df.sum()))\n",
    "\n",
    "y = success_df.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>U</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>membership_length</th>\n",
       "      <th>av_money_spent</th>\n",
       "      <th>num_received</th>\n",
       "      <th>viewed/received</th>\n",
       "      <th>...</th>\n",
       "      <th>offer_4</th>\n",
       "      <th>offer_5</th>\n",
       "      <th>offer_6</th>\n",
       "      <th>offer_7</th>\n",
       "      <th>offer_8</th>\n",
       "      <th>offer_9</th>\n",
       "      <th>informational</th>\n",
       "      <th>bogo</th>\n",
       "      <th>discount</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.193501</td>\n",
       "      <td>1.654475</td>\n",
       "      <td>0.132255</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011285</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792728</td>\n",
       "      <td>0.253302</td>\n",
       "      <td>-1.245015</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.253576</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620969</td>\n",
       "      <td>-0.540695</td>\n",
       "      <td>-0.865163</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     F    M    O    U       age    income  membership_length  av_money_spent  \\\n",
       "0  1.0  0.0  0.0  0.0  1.193501  1.654475           0.132255       -0.763102   \n",
       "1  0.0  0.0  0.0  1.0  0.000000  0.000000          -0.011285       -0.763102   \n",
       "2  0.0  1.0  0.0  0.0  0.792728  0.253302          -1.245015       -0.763102   \n",
       "3  0.0  0.0  0.0  1.0  0.000000  0.000000          -0.253576       -0.763102   \n",
       "4  0.0  1.0  0.0  0.0  0.620969 -0.540695          -0.865163       -0.763102   \n",
       "\n",
       "   num_received  viewed/received  ...  offer_4  offer_5  offer_6  offer_7  \\\n",
       "0     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "1     -1.206595        -1.292364  ...        1        0        0        0   \n",
       "2     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "3     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "4     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "\n",
       "   offer_8  offer_9  informational  bogo  discount  label  \n",
       "0        0        0              0     1         0      3  \n",
       "1        0        0              0     0         1      2  \n",
       "2        0        1              0     0         1      2  \n",
       "3        0        0              0     1         0      2  \n",
       "4        1        0              0     1         0      1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy weights: [0.7010725458066438, 1.4527769546105596]\n"
     ]
    }
   ],
   "source": [
    "# weighing factor for the cross entropy loss:\n",
    "num_success = succesful_mask.sum()\n",
    "num_unsuccess = len(succesful_mask.values) - num_success\n",
    "average = (num_success + num_unsuccess) / 2\n",
    "cross_entropy_weights = [average / num_unsuccess, average / num_success / 1.2]\n",
    "print('cross entropy weights: {}'.format(cross_entropy_weights))\n",
    "\n",
    "cross_entropy_weights = torch.as_tensor(cross_entropy_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (49671, 25)\n",
      "validation size: (5520, 25)\n",
      "test size: (6133, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, shuffle=True)\n",
    "print('training size: {}'.format(X_train.shape))\n",
    "print('validation size: {}'.format(X_val.shape))\n",
    "print('test size: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark kNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 867\n",
      "True Negatives: 3189\n",
      "False Positives: 861\n",
      "False Negatives: 1216\n",
      "accuracy: 66.13402902331649%\n",
      "precision: 50.173611111111114%\n",
      "recall: 41.62265962554009%\n",
      "false negative rate: 58.37734037445992%\n"
     ]
    }
   ],
   "source": [
    "model_kNN = KNeighborsClassifier(n_neighbors=5)\n",
    "model_kNN.fit(X_train, y_train)\n",
    "\n",
    "pred_labels = model_kNN.predict(X_test).astype(np.int8)\n",
    "true_labels = y_test.astype(np.int8)\n",
    "\n",
    "# binary classification performance measures:\n",
    "TP = (pred_labels & true_labels).sum()\n",
    "TN = ((1 - pred_labels) & (1 - true_labels)).sum()\n",
    "FP = (pred_labels & (1 - true_labels)).sum()\n",
    "FN = ((1 - pred_labels) & true_labels).sum()\n",
    "print('True Positives: {}'.format(TP))\n",
    "print('True Negatives: {}'.format(TN))\n",
    "print('False Positives: {}'.format(FP))\n",
    "print('False Negatives: {}'.format(FN))\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "false_negative_rate = FN / (TP + FN)\n",
    "print('accuracy: {}%'.format(accuracy * 100))\n",
    "print('precision: {}%'.format(precision * 100))\n",
    "print('recall: {}%'.format(recall * 100))\n",
    "print('false negative rate: {}%'.format(false_negative_rate * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# converting the data into torch.tensor:\n",
    "X_train = torch.as_tensor(X_train, dtype=torch.float).to(device)\n",
    "X_val = torch.as_tensor(X_val, dtype=torch.float).to(device)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float).to(device)\n",
    "\n",
    "y_train = torch.as_tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val = torch.as_tensor(y_val, dtype=torch.long).to(device)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# creating datasets for the dataloaders:\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 1024\n",
    "# creating the data loaders:\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining and training network for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters:\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 2\n",
    "hidden_dims = [256, 512, 512, 128]\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING\n",
      "Epoch 0/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 0/29 TRAIN loss/acc : 0.678/44.28%\n",
      "EPOCH 0/29 VAL loss/acc : 0.735/58.57%\n",
      "----------\n",
      "Epoch 1/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 1/29 TRAIN loss/acc : 0.665/55.94%\n",
      "EPOCH 1/29 VAL loss/acc : 0.717/59.93%\n",
      "----------\n",
      "Epoch 2/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 2/29 TRAIN loss/acc : 0.651/59.07%\n",
      "EPOCH 2/29 VAL loss/acc : 0.698/61.70%\n",
      "----------\n",
      "Epoch 3/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 3/29 TRAIN loss/acc : 0.642/60.31%\n",
      "EPOCH 3/29 VAL loss/acc : 0.685/63.82%\n",
      "----------\n",
      "Epoch 4/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 4/29 TRAIN loss/acc : 0.636/60.53%\n",
      "EPOCH 4/29 VAL loss/acc : 0.676/64.78%\n",
      "----------\n",
      "Epoch 5/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 5/29 TRAIN loss/acc : 0.633/61.25%\n",
      "EPOCH 5/29 VAL loss/acc : 0.673/64.58%\n",
      "----------\n",
      "Epoch 6/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 6/29 TRAIN loss/acc : 0.630/61.36%\n",
      "EPOCH 6/29 VAL loss/acc : 0.670/64.78%\n",
      "----------\n",
      "Epoch 7/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 7/29 TRAIN loss/acc : 0.629/61.43%\n",
      "EPOCH 7/29 VAL loss/acc : 0.669/65.02%\n",
      "----------\n",
      "Epoch 8/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 8/29 TRAIN loss/acc : 0.629/61.64%\n",
      "EPOCH 8/29 VAL loss/acc : 0.669/64.84%\n",
      "----------\n",
      "Epoch 9/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 9/29 TRAIN loss/acc : 0.628/61.60%\n",
      "EPOCH 9/29 VAL loss/acc : 0.662/65.11%\n",
      "----------\n",
      "Epoch 10/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 10/29 TRAIN loss/acc : 0.626/61.96%\n",
      "EPOCH 10/29 VAL loss/acc : 0.664/64.42%\n",
      "----------\n",
      "Epoch 11/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 11/29 TRAIN loss/acc : 0.627/61.36%\n",
      "EPOCH 11/29 VAL loss/acc : 0.665/64.71%\n",
      "----------\n",
      "Epoch 12/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 12/29 TRAIN loss/acc : 0.625/61.90%\n",
      "EPOCH 12/29 VAL loss/acc : 0.661/64.75%\n",
      "----------\n",
      "Epoch 13/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 13/29 TRAIN loss/acc : 0.626/61.48%\n",
      "EPOCH 13/29 VAL loss/acc : 0.666/65.04%\n",
      "----------\n",
      "Epoch 14/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 14/29 TRAIN loss/acc : 0.625/61.66%\n",
      "EPOCH 14/29 VAL loss/acc : 0.666/65.20%\n",
      "----------\n",
      "Epoch 15/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 15/29 TRAIN loss/acc : 0.624/61.89%\n",
      "EPOCH 15/29 VAL loss/acc : 0.662/64.60%\n",
      "----------\n",
      "Epoch 16/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 16/29 TRAIN loss/acc : 0.623/61.78%\n",
      "EPOCH 16/29 VAL loss/acc : 0.663/64.93%\n",
      "----------\n",
      "Epoch 17/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 17/29 TRAIN loss/acc : 0.622/61.82%\n",
      "EPOCH 17/29 VAL loss/acc : 0.660/65.60%\n",
      "----------\n",
      "Epoch 18/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 18/29 TRAIN loss/acc : 0.624/61.69%\n",
      "EPOCH 18/29 VAL loss/acc : 0.663/64.96%\n",
      "----------\n",
      "Epoch 19/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 19/29 TRAIN loss/acc : 0.621/61.88%\n",
      "EPOCH 19/29 VAL loss/acc : 0.660/64.76%\n",
      "----------\n",
      "Epoch 20/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 20/29 TRAIN loss/acc : 0.622/61.80%\n",
      "EPOCH 20/29 VAL loss/acc : 0.660/64.60%\n",
      "----------\n",
      "Epoch 21/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 21/29 TRAIN loss/acc : 0.622/61.71%\n",
      "EPOCH 21/29 VAL loss/acc : 0.662/64.76%\n",
      "----------\n",
      "Epoch 22/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 22/29 TRAIN loss/acc : 0.623/61.25%\n",
      "EPOCH 22/29 VAL loss/acc : 0.662/65.25%\n",
      "----------\n",
      "Epoch 23/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 23/29 TRAIN loss/acc : 0.621/62.07%\n",
      "EPOCH 23/29 VAL loss/acc : 0.663/65.31%\n",
      "----------\n",
      "Epoch 24/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 24/29 TRAIN loss/acc : 0.620/61.79%\n",
      "EPOCH 24/29 VAL loss/acc : 0.656/65.07%\n",
      "----------\n",
      "Epoch 25/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 25/29 TRAIN loss/acc : 0.620/61.76%\n",
      "EPOCH 25/29 VAL loss/acc : 0.660/65.05%\n",
      "----------\n",
      "Epoch 26/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 26/29 TRAIN loss/acc : 0.621/61.55%\n",
      "EPOCH 26/29 VAL loss/acc : 0.660/65.49%\n",
      "----------\n",
      "Epoch 27/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 27/29 TRAIN loss/acc : 0.621/61.53%\n",
      "EPOCH 27/29 VAL loss/acc : 0.658/65.69%\n",
      "----------\n",
      "Epoch 28/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 28/29 TRAIN loss/acc : 0.621/61.85%\n",
      "EPOCH 28/29 VAL loss/acc : 0.659/65.63%\n",
      "----------\n",
      "Epoch 29/29\n",
      "learning rate : 5e-05\n",
      "----------\n",
      "EPOCH 29/29 TRAIN loss/acc : 0.621/61.93%\n",
      "EPOCH 29/29 VAL loss/acc : 0.656/64.86%\n",
      "----------\n",
      "FINISH.\n",
      "Training complete in 1m 49s\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model:\n",
    "model_NN_binary = Linear_NN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim,\n",
    "                     dropout=dropout, activation='leaky_relu')\n",
    "model_NN_binary.to(device)\n",
    "\n",
    "saving_path = 'models/NN_model_binary.pth'\n",
    "\n",
    "NN_solver_binary = NN_Solver(optim_args={\"lr\": 5e-5, \"weight_decay\": 0},\n",
    "                loss_func=torch.nn.CrossEntropyLoss(weight=cross_entropy_weights))\n",
    "NN_solver_binary.train(model_NN_binary, train_loader, val_loader,\n",
    "                       log_nth=0, num_epochs=30, saving_path=saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy: 0.6568840579710145\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing the train and validation loss history:\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(NN_solver_binary.train_loss_history)\n",
    "plt.plot(NN_solver_binary.val_loss_history)\n",
    "\n",
    "print('best validation accuracy: {}'.format(max(NN_solver_binary.val_acc_history)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "After the accuracy of the binary classifier the model can be tested on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 1565\n",
      "True Negatives: 2424\n",
      "False Positives: 1626\n",
      "False Negatives: 518\n",
      "accuracy: 65.04157834664927%\n",
      "precision: 49.04418677530555%\n",
      "recall: 75.13202112337974%\n",
      "false negative rate: 24.86797887662026%\n"
     ]
    }
   ],
   "source": [
    "# testing the trained model:\n",
    "model_NN_binary.eval()\n",
    "output = model_NN_binary(X_test)\n",
    "pred_labels = output.argmax(dim=1).cpu().detach().numpy()\n",
    "true_labels = y_test.cpu().numpy()\n",
    "# binary classification performance measures:\n",
    "TP = (pred_labels & true_labels).sum()\n",
    "TN = ((1 - pred_labels) & (1 - true_labels)).sum()\n",
    "FP = (pred_labels & (1 - true_labels)).sum()\n",
    "FN = ((1 - pred_labels) & true_labels).sum()\n",
    "print('True Positives: {}'.format(TP))\n",
    "print('True Negatives: {}'.format(TN))\n",
    "print('False Positives: {}'.format(FP))\n",
    "print('False Negatives: {}'.format(FN))\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "false_negative_rate = FN / (TP + FN)\n",
    "print('accuracy: {}%'.format(accuracy * 100))\n",
    "print('precision: {}%'.format(precision * 100))\n",
    "print('recall: {}%'.format(recall * 100))\n",
    "print('false negative rate: {}%'.format(false_negative_rate * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0]\n",
      "real:      [1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# some sample labels:\n",
    "print('predicted: {}'.format(pred_labels[500: 520]))\n",
    "print('real:      {}'.format(true_labels[500: 520]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('starbucks': conda)",
   "language": "python",
   "name": "python38264bitstarbuckscondad9e38c9f842c41baadfb9dbf2b751888"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
