{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training notebook\n",
    "\n",
    "This notebook contains the training code for the Starbucks Capstone Challenge.\n",
    "\n",
    "It has got the following structure:\n",
    "* Checking the correlation of the features in the training data\n",
    "* Based on this seleceting the sufficient features\n",
    "* Shuffleing the data\n",
    "* splitting the data into train, validation and test sets\n",
    "* evaluating the banchmark model kNN\n",
    "* Training a feed forward neural network for classification with 4 classes:\n",
    "    * creating data loaders\n",
    "    * defining the model\n",
    "    * finding sufficient hyperparameters\n",
    "    * training the models\n",
    "    * evaluating the trained model\n",
    "* Training a feed forward neural network for binary classification:\n",
    "    * creating the binary labels from the 4 original classes\n",
    "    * creating data loaders\n",
    "    * defining the model\n",
    "    * finding sufficient hyperparameters\n",
    "    * training the models\n",
    "    * evaluating the trained model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from importlib import reload\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from source import training_helpers\n",
    "from source import model\n",
    "from source.model import Linear_NN\n",
    "from source import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is avalilabe\n"
     ]
    }
   ],
   "source": [
    "# setting up torch device:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('cuda is avalilabe' if torch.cuda.is_available() else 'cuda is NOT avaliable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(training_helpers)\n",
    "reload(model)\n",
    "reload(solver)\n",
    "from source.model import Linear_NN\n",
    "from source.solver import NN_Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the dataset and checking feature correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read training data from data/training_data_standardized.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>U</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>membership_length</th>\n",
       "      <th>av_money_spent</th>\n",
       "      <th>num_received</th>\n",
       "      <th>viewed/received</th>\n",
       "      <th>...</th>\n",
       "      <th>offer_4</th>\n",
       "      <th>offer_5</th>\n",
       "      <th>offer_6</th>\n",
       "      <th>offer_7</th>\n",
       "      <th>offer_8</th>\n",
       "      <th>offer_9</th>\n",
       "      <th>informational</th>\n",
       "      <th>bogo</th>\n",
       "      <th>discount</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.193501</td>\n",
       "      <td>1.654475</td>\n",
       "      <td>0.132255</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.011285</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792728</td>\n",
       "      <td>0.253302</td>\n",
       "      <td>-1.245015</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.253576</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620969</td>\n",
       "      <td>-0.540695</td>\n",
       "      <td>-0.865163</td>\n",
       "      <td>-0.763102</td>\n",
       "      <td>-1.206595</td>\n",
       "      <td>-1.292364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     F    M    O    U       age    income  membership_length  av_money_spent  \\\n",
       "0  1.0  0.0  0.0  0.0  1.193501  1.654475           0.132255       -0.763102   \n",
       "1  0.0  0.0  0.0  1.0  0.000000  0.000000          -0.011285       -0.763102   \n",
       "2  0.0  1.0  0.0  0.0  0.792728  0.253302          -1.245015       -0.763102   \n",
       "3  0.0  0.0  0.0  1.0  0.000000  0.000000          -0.253576       -0.763102   \n",
       "4  0.0  1.0  0.0  0.0  0.620969 -0.540695          -0.865163       -0.763102   \n",
       "\n",
       "   num_received  viewed/received  ...  offer_4  offer_5  offer_6  offer_7  \\\n",
       "0     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "1     -1.206595        -1.292364  ...        1        0        0        0   \n",
       "2     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "3     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "4     -1.206595        -1.292364  ...        0        0        0        0   \n",
       "\n",
       "   offer_8  offer_9  informational  bogo  discount  label  \n",
       "0        0        0              0     1         0      3  \n",
       "1        0        0              0     0         1      2  \n",
       "2        0        1              0     0         1      2  \n",
       "3        0        0              0     1         0      2  \n",
       "4        1        0              0     1         0      1  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data:\n",
    "data_dir = 'data'\n",
    "data_file = 'training_data_standardized.csv'\n",
    "training_data_df = training_helpers.load_training_data(data_dir=data_dir, data_file=data_file)\n",
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross entropy weights\n",
    "As it was seen in the feature engineering dataset, there are different amount of training samples for each class, so weighing during training is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f19806ccfd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# chceking the number of labels in the dataset:\n",
    "label_counts_df = training_data_df.label.value_counts().sort_index()\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "label_counts_df.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy weights of the classes: [0.9069451  1.41665127 0.76273632 1.1357979 ]\n"
     ]
    }
   ],
   "source": [
    "# creating weighing factor for the cross entropy loss:\n",
    "label_counts = training_data_df.label.value_counts().sort_index().values\n",
    "\n",
    "cross_entropy_weights = label_counts.mean() / label_counts\n",
    "print('cross entropy weights of the classes: {}'.format(cross_entropy_weights))\n",
    "# putting it into a tensor on cuda:\n",
    "cross_entropy_weights = torch.as_tensor(cross_entropy_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F</th>\n",
       "      <th>M</th>\n",
       "      <th>O</th>\n",
       "      <th>U</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>membership_length</th>\n",
       "      <th>av_money_spent</th>\n",
       "      <th>num_received</th>\n",
       "      <th>viewed/received</th>\n",
       "      <th>...</th>\n",
       "      <th>offer_4</th>\n",
       "      <th>offer_5</th>\n",
       "      <th>offer_6</th>\n",
       "      <th>offer_7</th>\n",
       "      <th>offer_8</th>\n",
       "      <th>offer_9</th>\n",
       "      <th>informational</th>\n",
       "      <th>bogo</th>\n",
       "      <th>discount</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.742123</td>\n",
       "      <td>-0.080854</td>\n",
       "      <td>-0.288567</td>\n",
       "      <td>0.144789</td>\n",
       "      <td>0.222408</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.198040</td>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006749</td>\n",
       "      <td>-0.004738</td>\n",
       "      <td>-0.004614</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.002159</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>0.004166</td>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.003275</td>\n",
       "      <td>0.053173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>-0.742123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>-0.393560</td>\n",
       "      <td>-0.137893</td>\n",
       "      <td>-0.210736</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>-0.035368</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>-0.027004</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002849</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>-0.014170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>-0.080854</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>-0.013558</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>-0.003873</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.016564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>-0.288567</td>\n",
       "      <td>-0.393560</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>-0.232751</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.033187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.002587</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>-0.003552</td>\n",
       "      <td>-0.059276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.144789</td>\n",
       "      <td>-0.137893</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300572</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.080883</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>-0.003986</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>0.029149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income</th>\n",
       "      <td>0.222408</td>\n",
       "      <td>-0.210736</td>\n",
       "      <td>-0.013558</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>0.300572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047885</td>\n",
       "      <td>0.244121</td>\n",
       "      <td>-0.006509</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.005584</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>-0.002103</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>-0.005300</td>\n",
       "      <td>0.061189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>membership_length</th>\n",
       "      <td>0.011123</td>\n",
       "      <td>-0.003784</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>-0.007125</td>\n",
       "      <td>0.020466</td>\n",
       "      <td>0.047885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.176122</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.005336</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>-0.005100</td>\n",
       "      <td>0.070948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>av_money_spent</th>\n",
       "      <td>0.198040</td>\n",
       "      <td>-0.035368</td>\n",
       "      <td>0.018863</td>\n",
       "      <td>-0.232751</td>\n",
       "      <td>0.080883</td>\n",
       "      <td>0.244121</td>\n",
       "      <td>0.176122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>0.396038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019673</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.029876</td>\n",
       "      <td>0.014563</td>\n",
       "      <td>-0.041691</td>\n",
       "      <td>0.077541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_received</th>\n",
       "      <td>-0.004785</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>-0.006509</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.313841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.516739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108061</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>-0.105950</td>\n",
       "      <td>0.067890</td>\n",
       "      <td>0.069505</td>\n",
       "      <td>-0.026006</td>\n",
       "      <td>0.107171</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>-0.150555</td>\n",
       "      <td>-0.023747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viewed/received</th>\n",
       "      <td>0.001956</td>\n",
       "      <td>-0.027004</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.033187</td>\n",
       "      <td>0.014495</td>\n",
       "      <td>0.028156</td>\n",
       "      <td>0.006887</td>\n",
       "      <td>0.396038</td>\n",
       "      <td>0.516739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035243</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>-0.042932</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>-0.005509</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>-0.048585</td>\n",
       "      <td>0.018606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed/viewed</th>\n",
       "      <td>0.112884</td>\n",
       "      <td>0.002391</td>\n",
       "      <td>0.013580</td>\n",
       "      <td>-0.166805</td>\n",
       "      <td>0.046993</td>\n",
       "      <td>0.096629</td>\n",
       "      <td>0.144729</td>\n",
       "      <td>0.530017</td>\n",
       "      <td>0.337040</td>\n",
       "      <td>0.414042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026592</td>\n",
       "      <td>-0.009568</td>\n",
       "      <td>-0.031542</td>\n",
       "      <td>0.020129</td>\n",
       "      <td>0.015268</td>\n",
       "      <td>-0.010146</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>0.017991</td>\n",
       "      <td>-0.044630</td>\n",
       "      <td>0.058924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed_not_viewed</th>\n",
       "      <td>0.126325</td>\n",
       "      <td>-0.025500</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.120203</td>\n",
       "      <td>0.062968</td>\n",
       "      <td>0.422587</td>\n",
       "      <td>0.198433</td>\n",
       "      <td>0.103660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019720</td>\n",
       "      <td>-0.003682</td>\n",
       "      <td>-0.021145</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.018375</td>\n",
       "      <td>0.008048</td>\n",
       "      <td>-0.024697</td>\n",
       "      <td>0.035829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_received_this</th>\n",
       "      <td>-0.003981</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.006976</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.097507</td>\n",
       "      <td>0.331111</td>\n",
       "      <td>0.176541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036359</td>\n",
       "      <td>-0.008941</td>\n",
       "      <td>-0.037468</td>\n",
       "      <td>0.025908</td>\n",
       "      <td>0.026121</td>\n",
       "      <td>-0.005022</td>\n",
       "      <td>0.037621</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>-0.049828</td>\n",
       "      <td>-0.007669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>viewed/received_this</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-0.009559</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.113534</td>\n",
       "      <td>0.283436</td>\n",
       "      <td>0.251604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071535</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>-0.015218</td>\n",
       "      <td>0.039592</td>\n",
       "      <td>0.055319</td>\n",
       "      <td>-0.037366</td>\n",
       "      <td>0.010424</td>\n",
       "      <td>0.048791</td>\n",
       "      <td>-0.059975</td>\n",
       "      <td>0.043009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed/viewed_this</th>\n",
       "      <td>0.037950</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>-0.057935</td>\n",
       "      <td>0.019797</td>\n",
       "      <td>0.035328</td>\n",
       "      <td>0.040501</td>\n",
       "      <td>0.171388</td>\n",
       "      <td>0.134594</td>\n",
       "      <td>0.138102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024854</td>\n",
       "      <td>0.053831</td>\n",
       "      <td>0.030239</td>\n",
       "      <td>-0.069729</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>-0.105789</td>\n",
       "      <td>0.056776</td>\n",
       "      <td>0.035021</td>\n",
       "      <td>0.069398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>completed_not_viewed_this</th>\n",
       "      <td>0.037784</td>\n",
       "      <td>-0.005542</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>-0.044390</td>\n",
       "      <td>0.015882</td>\n",
       "      <td>0.031224</td>\n",
       "      <td>0.019331</td>\n",
       "      <td>0.126682</td>\n",
       "      <td>0.097160</td>\n",
       "      <td>0.036770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003839</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>-0.016795</td>\n",
       "      <td>-0.046930</td>\n",
       "      <td>0.026260</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>-0.071200</td>\n",
       "      <td>0.044032</td>\n",
       "      <td>0.017521</td>\n",
       "      <td>0.023979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_0</th>\n",
       "      <td>-0.000108</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>-0.002198</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>-0.010350</td>\n",
       "      <td>-0.034567</td>\n",
       "      <td>-0.011767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094740</td>\n",
       "      <td>-0.106043</td>\n",
       "      <td>-0.092320</td>\n",
       "      <td>-0.118282</td>\n",
       "      <td>-0.117219</td>\n",
       "      <td>-0.106820</td>\n",
       "      <td>-0.179451</td>\n",
       "      <td>0.380995</td>\n",
       "      <td>-0.236474</td>\n",
       "      <td>0.073736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_1</th>\n",
       "      <td>0.002295</td>\n",
       "      <td>-0.001056</td>\n",
       "      <td>-0.008012</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>-0.002082</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.005271</td>\n",
       "      <td>0.013244</td>\n",
       "      <td>0.073504</td>\n",
       "      <td>0.021825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104960</td>\n",
       "      <td>-0.117482</td>\n",
       "      <td>-0.102279</td>\n",
       "      <td>-0.131042</td>\n",
       "      <td>-0.129864</td>\n",
       "      <td>-0.118344</td>\n",
       "      <td>-0.198810</td>\n",
       "      <td>0.422095</td>\n",
       "      <td>-0.261984</td>\n",
       "      <td>0.072617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_2</th>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>-0.003135</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.073351</td>\n",
       "      <td>0.022699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105388</td>\n",
       "      <td>-0.117960</td>\n",
       "      <td>-0.102695</td>\n",
       "      <td>-0.131575</td>\n",
       "      <td>-0.130393</td>\n",
       "      <td>-0.118825</td>\n",
       "      <td>0.658765</td>\n",
       "      <td>-0.310282</td>\n",
       "      <td>-0.263050</td>\n",
       "      <td>-0.220835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_3</th>\n",
       "      <td>-0.000737</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>-0.002300</td>\n",
       "      <td>-0.003768</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>-0.030938</td>\n",
       "      <td>-0.009611</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095104</td>\n",
       "      <td>-0.106451</td>\n",
       "      <td>-0.092675</td>\n",
       "      <td>-0.118736</td>\n",
       "      <td>-0.117670</td>\n",
       "      <td>-0.107231</td>\n",
       "      <td>-0.180141</td>\n",
       "      <td>0.382460</td>\n",
       "      <td>-0.237383</td>\n",
       "      <td>-0.051766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_4</th>\n",
       "      <td>0.006749</td>\n",
       "      <td>-0.002849</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>-0.001267</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>-0.019673</td>\n",
       "      <td>-0.108061</td>\n",
       "      <td>-0.035243</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094535</td>\n",
       "      <td>-0.082302</td>\n",
       "      <td>-0.105446</td>\n",
       "      <td>-0.104499</td>\n",
       "      <td>-0.095229</td>\n",
       "      <td>-0.159977</td>\n",
       "      <td>-0.248665</td>\n",
       "      <td>0.400637</td>\n",
       "      <td>-0.116674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_5</th>\n",
       "      <td>-0.004738</td>\n",
       "      <td>0.003803</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>-0.000318</td>\n",
       "      <td>-0.010036</td>\n",
       "      <td>-0.011763</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.092121</td>\n",
       "      <td>-0.118026</td>\n",
       "      <td>-0.116966</td>\n",
       "      <td>-0.106590</td>\n",
       "      <td>-0.179063</td>\n",
       "      <td>-0.278331</td>\n",
       "      <td>0.448433</td>\n",
       "      <td>0.140002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_6</th>\n",
       "      <td>-0.004614</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>-0.001151</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.005584</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>-0.105950</td>\n",
       "      <td>-0.042932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082302</td>\n",
       "      <td>-0.092121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.102753</td>\n",
       "      <td>-0.101829</td>\n",
       "      <td>-0.092796</td>\n",
       "      <td>-0.155891</td>\n",
       "      <td>-0.242313</td>\n",
       "      <td>0.390403</td>\n",
       "      <td>0.148012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_7</th>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>0.002963</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.005979</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.021134</td>\n",
       "      <td>0.067890</td>\n",
       "      <td>0.019774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.105446</td>\n",
       "      <td>-0.118026</td>\n",
       "      <td>-0.102753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.130465</td>\n",
       "      <td>-0.118891</td>\n",
       "      <td>0.659131</td>\n",
       "      <td>-0.310455</td>\n",
       "      <td>-0.263196</td>\n",
       "      <td>-0.069124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_8</th>\n",
       "      <td>-0.002159</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>-0.005336</td>\n",
       "      <td>0.016454</td>\n",
       "      <td>0.069505</td>\n",
       "      <td>0.027679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104499</td>\n",
       "      <td>-0.116966</td>\n",
       "      <td>-0.101829</td>\n",
       "      <td>-0.130465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.117823</td>\n",
       "      <td>-0.197935</td>\n",
       "      <td>0.420239</td>\n",
       "      <td>-0.260832</td>\n",
       "      <td>0.104721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer_9</th>\n",
       "      <td>-0.002574</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>-0.002334</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>-0.002103</td>\n",
       "      <td>-0.000274</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>-0.026006</td>\n",
       "      <td>-0.005509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095229</td>\n",
       "      <td>-0.106590</td>\n",
       "      <td>-0.092796</td>\n",
       "      <td>-0.118891</td>\n",
       "      <td>-0.117823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.180376</td>\n",
       "      <td>-0.280372</td>\n",
       "      <td>0.451722</td>\n",
       "      <td>-0.063379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informational</th>\n",
       "      <td>0.004166</td>\n",
       "      <td>-0.002820</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>-0.002587</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.029876</td>\n",
       "      <td>0.107171</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.159977</td>\n",
       "      <td>-0.179063</td>\n",
       "      <td>-0.155891</td>\n",
       "      <td>0.659131</td>\n",
       "      <td>-0.197935</td>\n",
       "      <td>-0.180376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.471006</td>\n",
       "      <td>-0.399308</td>\n",
       "      <td>-0.219992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bogo</th>\n",
       "      <td>-0.000412</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>-0.003873</td>\n",
       "      <td>0.005630</td>\n",
       "      <td>-0.003986</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.014563</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>0.019186</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.248665</td>\n",
       "      <td>-0.278331</td>\n",
       "      <td>-0.242313</td>\n",
       "      <td>-0.310455</td>\n",
       "      <td>0.420239</td>\n",
       "      <td>-0.280372</td>\n",
       "      <td>-0.471006</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.620675</td>\n",
       "      <td>0.127475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discount</th>\n",
       "      <td>-0.003275</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>-0.003552</td>\n",
       "      <td>-0.002729</td>\n",
       "      <td>-0.005300</td>\n",
       "      <td>-0.005100</td>\n",
       "      <td>-0.041691</td>\n",
       "      <td>-0.150555</td>\n",
       "      <td>-0.048585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400637</td>\n",
       "      <td>0.448433</td>\n",
       "      <td>0.390403</td>\n",
       "      <td>-0.263196</td>\n",
       "      <td>-0.260832</td>\n",
       "      <td>0.451722</td>\n",
       "      <td>-0.399308</td>\n",
       "      <td>-0.620675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>0.053173</td>\n",
       "      <td>-0.014170</td>\n",
       "      <td>0.016564</td>\n",
       "      <td>-0.059276</td>\n",
       "      <td>0.029149</td>\n",
       "      <td>0.061189</td>\n",
       "      <td>0.070948</td>\n",
       "      <td>0.077541</td>\n",
       "      <td>-0.023747</td>\n",
       "      <td>0.018606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116674</td>\n",
       "      <td>0.140002</td>\n",
       "      <td>0.148012</td>\n",
       "      <td>-0.069124</td>\n",
       "      <td>0.104721</td>\n",
       "      <td>-0.063379</td>\n",
       "      <td>-0.219992</td>\n",
       "      <td>0.127475</td>\n",
       "      <td>0.063048</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  F         M         O         U       age  \\\n",
       "F                          1.000000 -0.742123 -0.080854 -0.288567  0.144789   \n",
       "M                         -0.742123  1.000000 -0.110272 -0.393560 -0.137893   \n",
       "O                         -0.080854 -0.110272  1.000000 -0.042878 -0.003326   \n",
       "U                         -0.288567 -0.393560 -0.042878  1.000000  0.000406   \n",
       "age                        0.144789 -0.137893 -0.003326  0.000406  1.000000   \n",
       "income                     0.222408 -0.210736 -0.013558  0.001730  0.300572   \n",
       "membership_length          0.011123 -0.003784 -0.009273 -0.007125  0.020466   \n",
       "av_money_spent             0.198040 -0.035368  0.018863 -0.232751  0.080883   \n",
       "num_received              -0.004785  0.002142 -0.004134  0.004901 -0.001565   \n",
       "viewed/received            0.001956 -0.027004  0.012073  0.033187  0.014495   \n",
       "completed/viewed           0.112884  0.002391  0.013580 -0.166805  0.046993   \n",
       "completed_not_viewed       0.126325 -0.025500  0.008031 -0.142857  0.044466   \n",
       "num_received_this         -0.003981 -0.000959  0.000132  0.006976  0.005968   \n",
       "viewed/received_this      -0.000007 -0.009559  0.002939  0.013158  0.005989   \n",
       "completed/viewed_this      0.037950  0.000828  0.010274 -0.057935  0.019797   \n",
       "completed_not_viewed_this  0.037784 -0.005542 -0.002022 -0.044390  0.015882   \n",
       "offer_0                   -0.000108 -0.001358  0.002588  0.001328 -0.002198   \n",
       "offer_1                    0.002295 -0.001056 -0.008012  0.000882 -0.002082   \n",
       "offer_2                    0.001120  0.000916  0.000654 -0.003135  0.004210   \n",
       "offer_3                   -0.000737 -0.000871  0.001314  0.001902 -0.002300   \n",
       "offer_4                    0.006749 -0.002849  0.002979 -0.006255  0.002606   \n",
       "offer_5                   -0.004738  0.003803  0.002733  0.000195 -0.000767   \n",
       "offer_6                   -0.004614  0.005320 -0.000603 -0.001151 -0.000065   \n",
       "offer_7                    0.004370 -0.004631  0.002963 -0.000275  0.005979   \n",
       "offer_8                   -0.002159 -0.000889 -0.001584  0.004855  0.000075   \n",
       "offer_9                   -0.002574  0.002432 -0.002334  0.000786 -0.005924   \n",
       "informational              0.004166 -0.002820  0.002745 -0.002587  0.007731   \n",
       "bogo                      -0.000412 -0.002588 -0.003873  0.005630 -0.003986   \n",
       "discount                  -0.003275  0.005196  0.001585 -0.003552 -0.002729   \n",
       "label                      0.053173 -0.014170  0.016564 -0.059276  0.029149   \n",
       "\n",
       "                             income  membership_length  av_money_spent  \\\n",
       "F                          0.222408           0.011123        0.198040   \n",
       "M                         -0.210736          -0.003784       -0.035368   \n",
       "O                         -0.013558          -0.009273        0.018863   \n",
       "U                          0.001730          -0.007125       -0.232751   \n",
       "age                        0.300572           0.020466        0.080883   \n",
       "income                     1.000000           0.047885        0.244121   \n",
       "membership_length          0.047885           1.000000        0.176122   \n",
       "av_money_spent             0.244121           0.176122        1.000000   \n",
       "num_received              -0.006509           0.000882        0.313841   \n",
       "viewed/received            0.028156           0.006887        0.396038   \n",
       "completed/viewed           0.096629           0.144729        0.530017   \n",
       "completed_not_viewed       0.120203           0.062968        0.422587   \n",
       "num_received_this         -0.000113          -0.000080        0.097507   \n",
       "viewed/received_this       0.006739           0.002575        0.113534   \n",
       "completed/viewed_this      0.035328           0.040501        0.171388   \n",
       "completed_not_viewed_this  0.031224           0.019331        0.126682   \n",
       "offer_0                    0.001645           0.000790       -0.010350   \n",
       "offer_1                    0.003162           0.005271        0.013244   \n",
       "offer_2                    0.000764           0.000403        0.018239   \n",
       "offer_3                   -0.003768           0.004939        0.002616   \n",
       "offer_4                   -0.001267           0.000589       -0.019673   \n",
       "offer_5                   -0.000318          -0.010036       -0.011763   \n",
       "offer_6                   -0.005584           0.001721       -0.036415   \n",
       "offer_7                    0.003583           0.001926        0.021134   \n",
       "offer_8                    0.002338          -0.005336        0.016454   \n",
       "offer_9                   -0.002103          -0.000274       -0.005191   \n",
       "informational              0.003298           0.001767        0.029876   \n",
       "bogo                       0.002279           0.003396        0.014563   \n",
       "discount                  -0.005300          -0.005100       -0.041691   \n",
       "label                      0.061189           0.070948        0.077541   \n",
       "\n",
       "                           num_received  viewed/received  ...   offer_4  \\\n",
       "F                             -0.004785         0.001956  ...  0.006749   \n",
       "M                              0.002142        -0.027004  ... -0.002849   \n",
       "O                             -0.004134         0.012073  ...  0.002979   \n",
       "U                              0.004901         0.033187  ... -0.006255   \n",
       "age                           -0.001565         0.014495  ...  0.002606   \n",
       "income                        -0.006509         0.028156  ... -0.001267   \n",
       "membership_length              0.000882         0.006887  ...  0.000589   \n",
       "av_money_spent                 0.313841         0.396038  ... -0.019673   \n",
       "num_received                   1.000000         0.516739  ... -0.108061   \n",
       "viewed/received                0.516739         1.000000  ... -0.035243   \n",
       "completed/viewed               0.337040         0.414042  ... -0.026592   \n",
       "completed_not_viewed           0.198433         0.103660  ... -0.019720   \n",
       "num_received_this              0.331111         0.176541  ... -0.036359   \n",
       "viewed/received_this           0.283436         0.251604  ... -0.071535   \n",
       "completed/viewed_this          0.134594         0.138102  ... -0.024854   \n",
       "completed_not_viewed_this      0.097160         0.036770  ...  0.003839   \n",
       "offer_0                       -0.034567        -0.011767  ... -0.094740   \n",
       "offer_1                        0.073504         0.021825  ... -0.104960   \n",
       "offer_2                        0.073351         0.022699  ... -0.105388   \n",
       "offer_3                       -0.030938        -0.009611  ... -0.095104   \n",
       "offer_4                       -0.108061        -0.035243  ...  1.000000   \n",
       "offer_5                       -0.024229        -0.002591  ... -0.094535   \n",
       "offer_6                       -0.105950        -0.042932  ... -0.082302   \n",
       "offer_7                        0.067890         0.019774  ... -0.105446   \n",
       "offer_8                        0.069505         0.027679  ... -0.104499   \n",
       "offer_9                       -0.026006        -0.005509  ... -0.095229   \n",
       "informational                  0.107171         0.032227  ... -0.159977   \n",
       "bogo                           0.053206         0.019186  ... -0.248665   \n",
       "discount                      -0.150555        -0.048585  ...  0.400637   \n",
       "label                         -0.023747         0.018606  ... -0.116674   \n",
       "\n",
       "                            offer_5   offer_6   offer_7   offer_8   offer_9  \\\n",
       "F                         -0.004738 -0.004614  0.004370 -0.002159 -0.002574   \n",
       "M                          0.003803  0.005320 -0.004631 -0.000889  0.002432   \n",
       "O                          0.002733 -0.000603  0.002963 -0.001584 -0.002334   \n",
       "U                          0.000195 -0.001151 -0.000275  0.004855  0.000786   \n",
       "age                       -0.000767 -0.000065  0.005979  0.000075 -0.005924   \n",
       "income                    -0.000318 -0.005584  0.003583  0.002338 -0.002103   \n",
       "membership_length         -0.010036  0.001721  0.001926 -0.005336 -0.000274   \n",
       "av_money_spent            -0.011763 -0.036415  0.021134  0.016454 -0.005191   \n",
       "num_received              -0.024229 -0.105950  0.067890  0.069505 -0.026006   \n",
       "viewed/received           -0.002591 -0.042932  0.019774  0.027679 -0.005509   \n",
       "completed/viewed          -0.009568 -0.031542  0.020129  0.015268 -0.010146   \n",
       "completed_not_viewed      -0.003682 -0.021145  0.011961  0.012478  0.000584   \n",
       "num_received_this         -0.008941 -0.037468  0.025908  0.026121 -0.005022   \n",
       "viewed/received_this       0.019414 -0.015218  0.039592  0.055319 -0.037366   \n",
       "completed/viewed_this      0.053831  0.030239 -0.069729  0.052714 -0.001545   \n",
       "completed_not_viewed_this  0.002604 -0.016795 -0.046930  0.026260  0.036958   \n",
       "offer_0                   -0.106043 -0.092320 -0.118282 -0.117219 -0.106820   \n",
       "offer_1                   -0.117482 -0.102279 -0.131042 -0.129864 -0.118344   \n",
       "offer_2                   -0.117960 -0.102695 -0.131575 -0.130393 -0.118825   \n",
       "offer_3                   -0.106451 -0.092675 -0.118736 -0.117670 -0.107231   \n",
       "offer_4                   -0.094535 -0.082302 -0.105446 -0.104499 -0.095229   \n",
       "offer_5                    1.000000 -0.092121 -0.118026 -0.116966 -0.106590   \n",
       "offer_6                   -0.092121  1.000000 -0.102753 -0.101829 -0.092796   \n",
       "offer_7                   -0.118026 -0.102753  1.000000 -0.130465 -0.118891   \n",
       "offer_8                   -0.116966 -0.101829 -0.130465  1.000000 -0.117823   \n",
       "offer_9                   -0.106590 -0.092796 -0.118891 -0.117823  1.000000   \n",
       "informational             -0.179063 -0.155891  0.659131 -0.197935 -0.180376   \n",
       "bogo                      -0.278331 -0.242313 -0.310455  0.420239 -0.280372   \n",
       "discount                   0.448433  0.390403 -0.263196 -0.260832  0.451722   \n",
       "label                      0.140002  0.148012 -0.069124  0.104721 -0.063379   \n",
       "\n",
       "                           informational      bogo  discount     label  \n",
       "F                               0.004166 -0.000412 -0.003275  0.053173  \n",
       "M                              -0.002820 -0.002588  0.005196 -0.014170  \n",
       "O                               0.002745 -0.003873  0.001585  0.016564  \n",
       "U                              -0.002587  0.005630 -0.003552 -0.059276  \n",
       "age                             0.007731 -0.003986 -0.002729  0.029149  \n",
       "income                          0.003298  0.002279 -0.005300  0.061189  \n",
       "membership_length               0.001767  0.003396 -0.005100  0.070948  \n",
       "av_money_spent                  0.029876  0.014563 -0.041691  0.077541  \n",
       "num_received                    0.107171  0.053206 -0.150555 -0.023747  \n",
       "viewed/received                 0.032227  0.019186 -0.048585  0.018606  \n",
       "completed/viewed                0.029175  0.017991 -0.044630  0.058924  \n",
       "completed_not_viewed            0.018375  0.008048 -0.024697  0.035829  \n",
       "num_received_this               0.037621  0.015769 -0.049828 -0.007669  \n",
       "viewed/received_this            0.010424  0.048791 -0.059975  0.043009  \n",
       "completed/viewed_this          -0.105789  0.056776  0.035021  0.069398  \n",
       "completed_not_viewed_this      -0.071200  0.044032  0.017521  0.023979  \n",
       "offer_0                        -0.179451  0.380995 -0.236474  0.073736  \n",
       "offer_1                        -0.198810  0.422095 -0.261984  0.072617  \n",
       "offer_2                         0.658765 -0.310282 -0.263050 -0.220835  \n",
       "offer_3                        -0.180141  0.382460 -0.237383 -0.051766  \n",
       "offer_4                        -0.159977 -0.248665  0.400637 -0.116674  \n",
       "offer_5                        -0.179063 -0.278331  0.448433  0.140002  \n",
       "offer_6                        -0.155891 -0.242313  0.390403  0.148012  \n",
       "offer_7                         0.659131 -0.310455 -0.263196 -0.069124  \n",
       "offer_8                        -0.197935  0.420239 -0.260832  0.104721  \n",
       "offer_9                        -0.180376 -0.280372  0.451722 -0.063379  \n",
       "informational                   1.000000 -0.471006 -0.399308 -0.219992  \n",
       "bogo                           -0.471006  1.000000 -0.620675  0.127475  \n",
       "discount                       -0.399308 -0.620675  1.000000  0.063048  \n",
       "label                          -0.219992  0.127475  0.063048  1.000000  \n",
       "\n",
       "[30 rows x 30 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the correlation matrix:\n",
    "training_data_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features does not seem to be correlated to eachother"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the training features and the labels\n",
    "\n",
    "It is possible to use all the data features for inference, however it is possible that some features are not that meaningful as others. For this in the following cells two kinds of methods is implemented. In the first one one is able to load the total feature vector. In the second one only some of the features are loaded, the ones which were chosen by column name in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell has to be run to generate the training set if all of the features would like to be used for training.\n",
    "X = training_data_df.values[:, :-1].astype(np.float32)\n",
    "y = training_data_df.values[:, -1].astype(np.float32)\n",
    "\n",
    "# deleting the original dataframe:\n",
    "#training_data_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F', 'M', 'O', 'U', 'age', 'income', 'membership_length',\n",
       "       'av_money_spent', 'num_received', 'viewed/received', 'completed/viewed',\n",
       "       'completed_not_viewed', 'num_received_this', 'viewed/received_this',\n",
       "       'completed/viewed_this', 'completed_not_viewed_this', 'offer_0',\n",
       "       'offer_1', 'offer_2', 'offer_3', 'offer_4', 'offer_5', 'offer_6',\n",
       "       'offer_7', 'offer_8', 'offer_9', 'informational', 'bogo', 'discount',\n",
       "       'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By generating the training data with this cell, only some training features are going to be used.\n",
    "# selecting only some features:\n",
    "columns = ['F', 'M', 'O', 'U', 'age', 'income', 'membership_length',\n",
    "           'av_money_spent', 'num_received', 'viewed/received', 'completed/viewed', \n",
    "           'num_received_this', 'viewed/received_this', 'completed/viewed_this', 'completed_not_viewed_this',\n",
    "           'offer_0', 'offer_1', 'offer_2', 'offer_3', 'offer_4', 'offer_5', 'offer_6',\n",
    "           'offer_7', 'offer_8', 'offer_9']\n",
    "X = training_data_df.loc[:, columns].values.astype(np.float32)\n",
    "y = training_data_df.label.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffleing and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (49671, 25)\n",
      "validation size: (5520, 25)\n",
      "test size: (6133, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=0, shuffle=True)\n",
    "print('training size: {}'.format(X_train.shape))\n",
    "print('validation size: {}'.format(X_val.shape))\n",
    "print('test size: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the acquirable accuracy with kNN\n",
    "\n",
    "Our banchmark model is a kNN classifier (k Nearest Neighbours). We are going to use sklearn's KNeighborsClassifier for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN validation accuracy: 0.463768115942029\n"
     ]
    }
   ],
   "source": [
    "model_kNN = KNeighborsClassifier(n_neighbors=10)\n",
    "model_kNN.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = model_kNN.predict(X_val)\n",
    "\n",
    "accuracy_val_kNN = np.mean(y_val_pred == y_val)\n",
    "print('kNN validation accuracy: {}'.format(accuracy_val_kNN))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a training a feed forward neural network.\n",
    "\n",
    "The network implementation can be found in `source/model.py`.\n",
    "\n",
    "The solver implementation can be founf in `source.solver.py`\n",
    "\n",
    "* First the data loaders are created.\n",
    "* we check the implementation by overfitting to a single data instance\n",
    "* train the feed forward model for 4 class classification\n",
    "* train the feed forward model for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the data into torch.tensor:\n",
    "X_train = torch.as_tensor(X_train, dtype=torch.float).to(device)\n",
    "X_val = torch.as_tensor(X_val, dtype=torch.float).to(device)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float).to(device)\n",
    "\n",
    "y_train = torch.as_tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val = torch.as_tensor(y_val, dtype=torch.long).to(device)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating datasets for the dataloaders:\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 512\n",
    "# creating the data loaders:\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting to a single data instance\n",
    "First we are checking the implementation by overfitting ot a single data instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data loaders for a single training insctance:\n",
    "X_train_single = torch.as_tensor(X_train[0, :].reshape(1, -1), dtype=torch.float).to(device)\n",
    "X_val_single = torch.as_tensor(X_val[0, :].reshape(1, -1), dtype=torch.float).to(device)\n",
    "\n",
    "y_train_single = torch.as_tensor(y_train[0].reshape(1), dtype=torch.long).to(device)\n",
    "y_val_single = torch.as_tensor(y_val[0].reshape(1), dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters:\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 4\n",
    "hidden_dims = [64]\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING\n",
      "Epoch 0/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "Iteration 0/1 train accuracy: 0.0, train_loss: 1.3149421215057373\n",
      "EPOCH 0/29 TRAIN loss/acc : 1.315/0.00%\n",
      "EPOCH 0/29 VAL loss/acc : 1.584/0.00%\n",
      "----------\n",
      "Epoch 1/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 1/29 TRAIN loss/acc : 1.189/100.00%\n",
      "EPOCH 1/29 VAL loss/acc : 1.584/0.00%\n",
      "----------\n",
      "Epoch 2/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 2/29 TRAIN loss/acc : 1.067/100.00%\n",
      "EPOCH 2/29 VAL loss/acc : 1.585/0.00%\n",
      "----------\n",
      "Epoch 3/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 3/29 TRAIN loss/acc : 0.965/100.00%\n",
      "EPOCH 3/29 VAL loss/acc : 1.586/0.00%\n",
      "----------\n",
      "Epoch 4/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 4/29 TRAIN loss/acc : 0.889/100.00%\n",
      "EPOCH 4/29 VAL loss/acc : 1.587/0.00%\n",
      "----------\n",
      "Epoch 5/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 5/29 TRAIN loss/acc : 0.836/100.00%\n",
      "EPOCH 5/29 VAL loss/acc : 1.589/0.00%\n",
      "----------\n",
      "Epoch 6/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 6/29 TRAIN loss/acc : 0.803/100.00%\n",
      "EPOCH 6/29 VAL loss/acc : 1.590/0.00%\n",
      "----------\n",
      "Epoch 7/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 7/29 TRAIN loss/acc : 0.782/100.00%\n",
      "EPOCH 7/29 VAL loss/acc : 1.591/0.00%\n",
      "----------\n",
      "Epoch 8/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 8/29 TRAIN loss/acc : 0.769/100.00%\n",
      "EPOCH 8/29 VAL loss/acc : 1.592/0.00%\n",
      "----------\n",
      "Epoch 9/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 9/29 TRAIN loss/acc : 0.761/100.00%\n",
      "EPOCH 9/29 VAL loss/acc : 1.594/0.00%\n",
      "----------\n",
      "Epoch 10/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 10/29 TRAIN loss/acc : 0.756/100.00%\n",
      "EPOCH 10/29 VAL loss/acc : 1.595/0.00%\n",
      "----------\n",
      "Epoch 11/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 11/29 TRAIN loss/acc : 0.752/100.00%\n",
      "EPOCH 11/29 VAL loss/acc : 1.596/0.00%\n",
      "----------\n",
      "Epoch 12/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 12/29 TRAIN loss/acc : 0.750/100.00%\n",
      "EPOCH 12/29 VAL loss/acc : 1.597/0.00%\n",
      "----------\n",
      "Epoch 13/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 13/29 TRAIN loss/acc : 0.748/100.00%\n",
      "EPOCH 13/29 VAL loss/acc : 1.598/0.00%\n",
      "----------\n",
      "Epoch 14/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 14/29 TRAIN loss/acc : 0.747/100.00%\n",
      "EPOCH 14/29 VAL loss/acc : 1.599/0.00%\n",
      "----------\n",
      "Epoch 15/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 15/29 TRAIN loss/acc : 0.746/100.00%\n",
      "EPOCH 15/29 VAL loss/acc : 1.600/0.00%\n",
      "----------\n",
      "Epoch 16/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 16/29 TRAIN loss/acc : 0.746/100.00%\n",
      "EPOCH 16/29 VAL loss/acc : 1.601/0.00%\n",
      "----------\n",
      "Epoch 17/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 17/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 17/29 VAL loss/acc : 1.602/0.00%\n",
      "----------\n",
      "Epoch 18/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 18/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 18/29 VAL loss/acc : 1.603/0.00%\n",
      "----------\n",
      "Epoch 19/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 19/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 19/29 VAL loss/acc : 1.604/0.00%\n",
      "----------\n",
      "Epoch 20/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 20/29 TRAIN loss/acc : 0.745/100.00%\n",
      "EPOCH 20/29 VAL loss/acc : 1.604/0.00%\n",
      "----------\n",
      "Epoch 21/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 21/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 21/29 VAL loss/acc : 1.605/0.00%\n",
      "----------\n",
      "Epoch 22/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 22/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 22/29 VAL loss/acc : 1.606/0.00%\n",
      "----------\n",
      "Epoch 23/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 23/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 23/29 VAL loss/acc : 1.606/0.00%\n",
      "----------\n",
      "Epoch 24/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 24/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 24/29 VAL loss/acc : 1.607/0.00%\n",
      "----------\n",
      "Epoch 25/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 25/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 25/29 VAL loss/acc : 1.607/0.00%\n",
      "----------\n",
      "Epoch 26/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 26/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 26/29 VAL loss/acc : 1.608/0.00%\n",
      "----------\n",
      "Epoch 27/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 27/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 27/29 VAL loss/acc : 1.608/0.00%\n",
      "----------\n",
      "Epoch 28/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 28/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 28/29 VAL loss/acc : 1.609/0.00%\n",
      "----------\n",
      "Epoch 29/29\n",
      "learning rate : 0.005\n",
      "----------\n",
      "EPOCH 29/29 TRAIN loss/acc : 0.744/100.00%\n",
      "EPOCH 29/29 VAL loss/acc : 1.609/0.00%\n",
      "----------\n",
      "FINISH.\n",
      "Training complete in 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# testing the model by overfitting to a single data instance:\n",
    "train_dataset_single = TensorDataset(X_train_single, y_train_single)\n",
    "val_dataset_single = TensorDataset(X_val_single, y_val_single)\n",
    "train_loader_single = torch.utils.data.DataLoader(train_dataset_single,\n",
    "                                                batch_size=1, shuffle=False, num_workers=0)\n",
    "val_loader_single = torch.utils.data.DataLoader(val_dataset_single,\n",
    "                                              batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "test_model = Linear_NN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, dropout=dropout)\n",
    "test_model.to(device)\n",
    "NN_solver = NN_Solver(optim_args={\"lr\": 5e-3, \"weight_decay\": 0},\n",
    "                loss_func=torch.nn.CrossEntropyLoss())\n",
    "best_state_dict = NN_solver.train(test_model, train_loader_single, val_loader_single, log_nth=100, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing the train loss history:\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(NN_solver.train_loss_history)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on the whole dataset.\n",
    "\n",
    "After we have validated that the model learns, since it was able to overfit to a single data instance, we can train a more complex architecture on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# network parameters:\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 4\n",
    "hidden_dims = [256, 512, 512, 128]\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING\n",
      "Epoch 0/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 0/99 TRAIN loss/acc : 2.681/32.86%\n",
      "EPOCH 0/99 VAL loss/acc : 2.619/42.10%\n",
      "----------\n",
      "Epoch 1/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 1/99 TRAIN loss/acc : 2.578/39.77%\n",
      "EPOCH 1/99 VAL loss/acc : 2.514/47.25%\n",
      "----------\n",
      "Epoch 2/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 2/99 TRAIN loss/acc : 2.526/42.89%\n",
      "EPOCH 2/99 VAL loss/acc : 2.466/49.58%\n",
      "----------\n",
      "Epoch 3/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 3/99 TRAIN loss/acc : 2.511/44.35%\n",
      "EPOCH 3/99 VAL loss/acc : 2.452/49.53%\n",
      "----------\n",
      "Epoch 4/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 4/99 TRAIN loss/acc : 2.496/44.97%\n",
      "EPOCH 4/99 VAL loss/acc : 2.444/49.86%\n",
      "----------\n",
      "Epoch 5/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 5/99 TRAIN loss/acc : 2.494/44.95%\n",
      "EPOCH 5/99 VAL loss/acc : 2.438/49.73%\n",
      "----------\n",
      "Epoch 6/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 6/99 TRAIN loss/acc : 2.489/45.34%\n",
      "EPOCH 6/99 VAL loss/acc : 2.435/50.63%\n",
      "----------\n",
      "Epoch 7/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 7/99 TRAIN loss/acc : 2.489/45.56%\n",
      "EPOCH 7/99 VAL loss/acc : 2.435/50.91%\n",
      "----------\n",
      "Epoch 8/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 8/99 TRAIN loss/acc : 2.485/45.86%\n",
      "EPOCH 8/99 VAL loss/acc : 2.433/50.29%\n",
      "----------\n",
      "Epoch 9/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 9/99 TRAIN loss/acc : 2.479/46.10%\n",
      "EPOCH 9/99 VAL loss/acc : 2.432/50.67%\n",
      "----------\n",
      "Epoch 10/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 10/99 TRAIN loss/acc : 2.476/46.01%\n",
      "EPOCH 10/99 VAL loss/acc : 2.432/50.49%\n",
      "----------\n",
      "Epoch 11/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 11/99 TRAIN loss/acc : 2.479/46.15%\n",
      "EPOCH 11/99 VAL loss/acc : 2.429/51.07%\n",
      "----------\n",
      "Epoch 12/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 12/99 TRAIN loss/acc : 2.474/46.32%\n",
      "EPOCH 12/99 VAL loss/acc : 2.429/51.11%\n",
      "----------\n",
      "Epoch 13/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 13/99 TRAIN loss/acc : 2.477/46.11%\n",
      "EPOCH 13/99 VAL loss/acc : 2.427/50.83%\n",
      "----------\n",
      "Epoch 14/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 14/99 TRAIN loss/acc : 2.470/46.30%\n",
      "EPOCH 14/99 VAL loss/acc : 2.426/51.07%\n",
      "----------\n",
      "Epoch 15/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 15/99 TRAIN loss/acc : 2.476/46.54%\n",
      "EPOCH 15/99 VAL loss/acc : 2.426/50.58%\n",
      "----------\n",
      "Epoch 16/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 16/99 TRAIN loss/acc : 2.473/46.41%\n",
      "EPOCH 16/99 VAL loss/acc : 2.427/50.80%\n",
      "----------\n",
      "Epoch 17/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 17/99 TRAIN loss/acc : 2.471/46.48%\n",
      "EPOCH 17/99 VAL loss/acc : 2.428/50.16%\n",
      "----------\n",
      "Epoch 18/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 18/99 TRAIN loss/acc : 2.476/45.99%\n",
      "EPOCH 18/99 VAL loss/acc : 2.426/51.01%\n",
      "----------\n",
      "Epoch 19/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 19/99 TRAIN loss/acc : 2.470/46.69%\n",
      "EPOCH 19/99 VAL loss/acc : 2.426/50.96%\n",
      "----------\n",
      "Epoch 20/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 20/99 TRAIN loss/acc : 2.471/46.82%\n",
      "EPOCH 20/99 VAL loss/acc : 2.425/50.94%\n",
      "----------\n",
      "Epoch 21/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 21/99 TRAIN loss/acc : 2.473/46.70%\n",
      "EPOCH 21/99 VAL loss/acc : 2.426/50.54%\n",
      "----------\n",
      "Epoch 22/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 22/99 TRAIN loss/acc : 2.472/46.65%\n",
      "EPOCH 22/99 VAL loss/acc : 2.425/51.11%\n",
      "----------\n",
      "Epoch 23/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 23/99 TRAIN loss/acc : 2.468/46.88%\n",
      "EPOCH 23/99 VAL loss/acc : 2.423/51.43%\n",
      "----------\n",
      "Epoch 24/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 24/99 TRAIN loss/acc : 2.469/46.76%\n",
      "EPOCH 24/99 VAL loss/acc : 2.423/51.09%\n",
      "----------\n",
      "Epoch 25/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 25/99 TRAIN loss/acc : 2.464/46.84%\n",
      "EPOCH 25/99 VAL loss/acc : 2.423/50.83%\n",
      "----------\n",
      "Epoch 26/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 26/99 TRAIN loss/acc : 2.461/46.89%\n",
      "EPOCH 26/99 VAL loss/acc : 2.422/51.63%\n",
      "----------\n",
      "Epoch 27/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 27/99 TRAIN loss/acc : 2.466/47.03%\n",
      "EPOCH 27/99 VAL loss/acc : 2.423/51.38%\n",
      "----------\n",
      "Epoch 28/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 28/99 TRAIN loss/acc : 2.464/46.76%\n",
      "EPOCH 28/99 VAL loss/acc : 2.422/50.96%\n",
      "----------\n",
      "Epoch 29/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 29/99 TRAIN loss/acc : 2.467/46.67%\n",
      "EPOCH 29/99 VAL loss/acc : 2.423/51.32%\n",
      "----------\n",
      "Epoch 30/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 30/99 TRAIN loss/acc : 2.468/47.04%\n",
      "EPOCH 30/99 VAL loss/acc : 2.423/50.94%\n",
      "----------\n",
      "Epoch 31/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 31/99 TRAIN loss/acc : 2.464/46.85%\n",
      "EPOCH 31/99 VAL loss/acc : 2.423/50.94%\n",
      "----------\n",
      "Epoch 32/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 32/99 TRAIN loss/acc : 2.464/46.66%\n",
      "EPOCH 32/99 VAL loss/acc : 2.422/51.56%\n",
      "----------\n",
      "Epoch 33/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 33/99 TRAIN loss/acc : 2.456/47.00%\n",
      "EPOCH 33/99 VAL loss/acc : 2.420/51.58%\n",
      "----------\n",
      "Epoch 34/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 34/99 TRAIN loss/acc : 2.462/47.14%\n",
      "EPOCH 34/99 VAL loss/acc : 2.419/51.39%\n",
      "----------\n",
      "Epoch 35/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 35/99 TRAIN loss/acc : 2.464/46.78%\n",
      "EPOCH 35/99 VAL loss/acc : 2.421/51.45%\n",
      "----------\n",
      "Epoch 36/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 36/99 TRAIN loss/acc : 2.464/46.90%\n",
      "EPOCH 36/99 VAL loss/acc : 2.421/51.32%\n",
      "----------\n",
      "Epoch 37/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 37/99 TRAIN loss/acc : 2.461/47.00%\n",
      "EPOCH 37/99 VAL loss/acc : 2.422/51.09%\n",
      "----------\n",
      "Epoch 38/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 38/99 TRAIN loss/acc : 2.458/46.91%\n",
      "EPOCH 38/99 VAL loss/acc : 2.421/51.39%\n",
      "----------\n",
      "Epoch 39/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 39/99 TRAIN loss/acc : 2.463/46.96%\n",
      "EPOCH 39/99 VAL loss/acc : 2.421/51.58%\n",
      "----------\n",
      "Epoch 40/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 40/99 TRAIN loss/acc : 2.453/47.48%\n",
      "EPOCH 40/99 VAL loss/acc : 2.419/51.29%\n",
      "----------\n",
      "Epoch 41/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 41/99 TRAIN loss/acc : 2.463/47.16%\n",
      "EPOCH 41/99 VAL loss/acc : 2.419/51.45%\n",
      "----------\n",
      "Epoch 42/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 42/99 TRAIN loss/acc : 2.459/47.32%\n",
      "EPOCH 42/99 VAL loss/acc : 2.419/51.01%\n",
      "----------\n",
      "Epoch 43/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 43/99 TRAIN loss/acc : 2.458/46.80%\n",
      "EPOCH 43/99 VAL loss/acc : 2.421/51.41%\n",
      "----------\n",
      "Epoch 44/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 44/99 TRAIN loss/acc : 2.460/47.20%\n",
      "EPOCH 44/99 VAL loss/acc : 2.420/50.92%\n",
      "----------\n",
      "Epoch 45/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 45/99 TRAIN loss/acc : 2.461/47.21%\n",
      "EPOCH 45/99 VAL loss/acc : 2.420/51.41%\n",
      "----------\n",
      "Epoch 46/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 46/99 TRAIN loss/acc : 2.457/47.37%\n",
      "EPOCH 46/99 VAL loss/acc : 2.418/51.07%\n",
      "----------\n",
      "Epoch 47/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 47/99 TRAIN loss/acc : 2.461/47.33%\n",
      "EPOCH 47/99 VAL loss/acc : 2.419/51.30%\n",
      "----------\n",
      "Epoch 48/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 48/99 TRAIN loss/acc : 2.461/47.20%\n",
      "EPOCH 48/99 VAL loss/acc : 2.418/51.12%\n",
      "----------\n",
      "Epoch 49/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 49/99 TRAIN loss/acc : 2.462/47.07%\n",
      "EPOCH 49/99 VAL loss/acc : 2.418/51.41%\n",
      "----------\n",
      "Epoch 50/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 50/99 TRAIN loss/acc : 2.463/47.27%\n",
      "EPOCH 50/99 VAL loss/acc : 2.419/51.59%\n",
      "----------\n",
      "Epoch 51/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 51/99 TRAIN loss/acc : 2.459/47.32%\n",
      "EPOCH 51/99 VAL loss/acc : 2.419/51.67%\n",
      "----------\n",
      "Epoch 52/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 52/99 TRAIN loss/acc : 2.459/47.25%\n",
      "EPOCH 52/99 VAL loss/acc : 2.420/51.59%\n",
      "----------\n",
      "Epoch 53/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 53/99 TRAIN loss/acc : 2.460/47.44%\n",
      "EPOCH 53/99 VAL loss/acc : 2.419/51.38%\n",
      "----------\n",
      "Epoch 54/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 54/99 TRAIN loss/acc : 2.462/47.35%\n",
      "EPOCH 54/99 VAL loss/acc : 2.417/51.52%\n",
      "----------\n",
      "Epoch 55/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 55/99 TRAIN loss/acc : 2.458/47.29%\n",
      "EPOCH 55/99 VAL loss/acc : 2.417/51.74%\n",
      "----------\n",
      "Epoch 56/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 56/99 TRAIN loss/acc : 2.458/47.31%\n",
      "EPOCH 56/99 VAL loss/acc : 2.417/51.45%\n",
      "----------\n",
      "Epoch 57/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 57/99 TRAIN loss/acc : 2.447/47.44%\n",
      "EPOCH 57/99 VAL loss/acc : 2.419/51.76%\n",
      "----------\n",
      "Epoch 58/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 58/99 TRAIN loss/acc : 2.456/47.17%\n",
      "EPOCH 58/99 VAL loss/acc : 2.418/51.61%\n",
      "----------\n",
      "Epoch 59/99\n",
      "learning rate : 0.0001\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 59/99 TRAIN loss/acc : 2.459/47.01%\n",
      "EPOCH 59/99 VAL loss/acc : 2.419/51.49%\n",
      "----------\n",
      "Epoch 60/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 60/99 TRAIN loss/acc : 2.457/47.39%\n",
      "EPOCH 60/99 VAL loss/acc : 2.419/51.47%\n",
      "----------\n",
      "Epoch 61/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 61/99 TRAIN loss/acc : 2.456/47.45%\n",
      "EPOCH 61/99 VAL loss/acc : 2.419/51.47%\n",
      "----------\n",
      "Epoch 62/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 62/99 TRAIN loss/acc : 2.452/47.57%\n",
      "EPOCH 62/99 VAL loss/acc : 2.419/51.30%\n",
      "----------\n",
      "Epoch 63/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 63/99 TRAIN loss/acc : 2.453/47.38%\n",
      "EPOCH 63/99 VAL loss/acc : 2.420/51.05%\n",
      "----------\n",
      "Epoch 64/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 64/99 TRAIN loss/acc : 2.455/47.40%\n",
      "EPOCH 64/99 VAL loss/acc : 2.417/51.78%\n",
      "----------\n",
      "Epoch 65/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 65/99 TRAIN loss/acc : 2.458/47.00%\n",
      "EPOCH 65/99 VAL loss/acc : 2.416/51.59%\n",
      "----------\n",
      "Epoch 66/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 66/99 TRAIN loss/acc : 2.452/47.26%\n",
      "EPOCH 66/99 VAL loss/acc : 2.418/51.65%\n",
      "----------\n",
      "Epoch 67/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 67/99 TRAIN loss/acc : 2.456/47.21%\n",
      "EPOCH 67/99 VAL loss/acc : 2.419/51.41%\n",
      "----------\n",
      "Epoch 68/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 68/99 TRAIN loss/acc : 2.454/47.56%\n",
      "EPOCH 68/99 VAL loss/acc : 2.418/51.79%\n",
      "----------\n",
      "Epoch 69/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 69/99 TRAIN loss/acc : 2.448/47.69%\n",
      "EPOCH 69/99 VAL loss/acc : 2.417/52.07%\n",
      "----------\n",
      "Epoch 70/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 70/99 TRAIN loss/acc : 2.452/47.73%\n",
      "EPOCH 70/99 VAL loss/acc : 2.418/51.38%\n",
      "----------\n",
      "Epoch 71/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 71/99 TRAIN loss/acc : 2.453/47.63%\n",
      "EPOCH 71/99 VAL loss/acc : 2.417/51.45%\n",
      "----------\n",
      "Epoch 72/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 72/99 TRAIN loss/acc : 2.458/47.42%\n",
      "EPOCH 72/99 VAL loss/acc : 2.419/51.21%\n",
      "----------\n",
      "Epoch 73/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 73/99 TRAIN loss/acc : 2.454/47.47%\n",
      "EPOCH 73/99 VAL loss/acc : 2.418/51.50%\n",
      "----------\n",
      "Epoch 74/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 74/99 TRAIN loss/acc : 2.445/47.90%\n",
      "EPOCH 74/99 VAL loss/acc : 2.417/51.54%\n",
      "----------\n",
      "Epoch 75/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 75/99 TRAIN loss/acc : 2.453/47.44%\n",
      "EPOCH 75/99 VAL loss/acc : 2.416/51.45%\n",
      "----------\n",
      "Epoch 76/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 76/99 TRAIN loss/acc : 2.454/47.33%\n",
      "EPOCH 76/99 VAL loss/acc : 2.416/51.83%\n",
      "----------\n",
      "Epoch 77/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 77/99 TRAIN loss/acc : 2.455/47.47%\n",
      "EPOCH 77/99 VAL loss/acc : 2.417/51.68%\n",
      "----------\n",
      "Epoch 78/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 78/99 TRAIN loss/acc : 2.455/47.34%\n",
      "EPOCH 78/99 VAL loss/acc : 2.415/51.85%\n",
      "----------\n",
      "Epoch 79/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 79/99 TRAIN loss/acc : 2.452/47.70%\n",
      "EPOCH 79/99 VAL loss/acc : 2.417/51.85%\n",
      "----------\n",
      "Epoch 80/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 80/99 TRAIN loss/acc : 2.456/47.57%\n",
      "EPOCH 80/99 VAL loss/acc : 2.419/51.63%\n",
      "----------\n",
      "Epoch 81/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 81/99 TRAIN loss/acc : 2.454/47.42%\n",
      "EPOCH 81/99 VAL loss/acc : 2.418/51.54%\n",
      "----------\n",
      "Epoch 82/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 82/99 TRAIN loss/acc : 2.450/47.70%\n",
      "EPOCH 82/99 VAL loss/acc : 2.417/51.50%\n",
      "----------\n",
      "Epoch 83/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 83/99 TRAIN loss/acc : 2.450/47.77%\n",
      "EPOCH 83/99 VAL loss/acc : 2.417/51.45%\n",
      "----------\n",
      "Epoch 84/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 84/99 TRAIN loss/acc : 2.450/47.80%\n",
      "EPOCH 84/99 VAL loss/acc : 2.417/51.56%\n",
      "----------\n",
      "Epoch 85/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 85/99 TRAIN loss/acc : 2.450/47.55%\n",
      "EPOCH 85/99 VAL loss/acc : 2.417/51.85%\n",
      "----------\n",
      "Epoch 86/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 86/99 TRAIN loss/acc : 2.449/47.89%\n",
      "EPOCH 86/99 VAL loss/acc : 2.418/51.43%\n",
      "----------\n",
      "Epoch 87/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 87/99 TRAIN loss/acc : 2.453/47.80%\n",
      "EPOCH 87/99 VAL loss/acc : 2.416/51.41%\n",
      "----------\n",
      "Epoch 88/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 88/99 TRAIN loss/acc : 2.448/47.54%\n",
      "EPOCH 88/99 VAL loss/acc : 2.417/51.61%\n",
      "----------\n",
      "Epoch 89/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 89/99 TRAIN loss/acc : 2.446/47.84%\n",
      "EPOCH 89/99 VAL loss/acc : 2.418/51.54%\n",
      "----------\n",
      "Epoch 90/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 90/99 TRAIN loss/acc : 2.455/47.53%\n",
      "EPOCH 90/99 VAL loss/acc : 2.417/51.65%\n",
      "----------\n",
      "Epoch 91/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 91/99 TRAIN loss/acc : 2.458/47.67%\n",
      "EPOCH 91/99 VAL loss/acc : 2.415/51.78%\n",
      "----------\n",
      "Epoch 92/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 92/99 TRAIN loss/acc : 2.445/47.85%\n",
      "EPOCH 92/99 VAL loss/acc : 2.417/51.36%\n",
      "----------\n",
      "Epoch 93/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 93/99 TRAIN loss/acc : 2.447/47.98%\n",
      "EPOCH 93/99 VAL loss/acc : 2.417/51.36%\n",
      "----------\n",
      "Epoch 94/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 94/99 TRAIN loss/acc : 2.449/47.41%\n",
      "EPOCH 94/99 VAL loss/acc : 2.417/51.34%\n",
      "----------\n",
      "Epoch 95/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 95/99 TRAIN loss/acc : 2.455/47.55%\n",
      "EPOCH 95/99 VAL loss/acc : 2.417/51.52%\n",
      "----------\n",
      "Epoch 96/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 96/99 TRAIN loss/acc : 2.448/47.56%\n",
      "EPOCH 96/99 VAL loss/acc : 2.418/51.41%\n",
      "----------\n",
      "Epoch 97/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 97/99 TRAIN loss/acc : 2.447/47.79%\n",
      "EPOCH 97/99 VAL loss/acc : 2.417/51.47%\n",
      "----------\n",
      "Epoch 98/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 98/99 TRAIN loss/acc : 2.454/47.64%\n",
      "EPOCH 98/99 VAL loss/acc : 2.416/51.76%\n",
      "----------\n",
      "Epoch 99/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 99/99 TRAIN loss/acc : 2.450/47.59%\n",
      "EPOCH 99/99 VAL loss/acc : 2.417/51.72%\n",
      "----------\n",
      "FINISH.\n",
      "Training complete in 1m 25s\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model:\n",
    "model_NN = Linear_NN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim,\n",
    "                     dropout=dropout, activation='leaky_relu')\n",
    "model_NN.to(device)\n",
    "\n",
    "saving_path = 'models/NN_model.pth'\n",
    "\n",
    "NN_solver = NN_Solver(optim_args={\"lr\": 1e-4, \"weight_decay\": 0},\n",
    "            loss_func=torch.nn.CrossEntropyLoss(weight=cross_entropy_weights))\n",
    "NN_solver.train(model_NN, train_loader, val_loader, log_nth=0, num_epochs=100, saving_path=saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy: 0.5206521739130435\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing the train and validation loss history:\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(NN_solver.train_loss_history)\n",
    "plt.plot(NN_solver.val_loss_history)\n",
    "\n",
    "print('best validation accuracy: {}'.format(max(NN_solver.val_acc_history)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "After the hyper parameters were set with the help of the validation set, the mosel can be tested on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.5196478069460296\n"
     ]
    }
   ],
   "source": [
    "# testing the trained model:\n",
    "model_NN.eval()\n",
    "output = model_NN(X_test)\n",
    "pred_labels = output.argmax(dim=1)\n",
    "\n",
    "test_accuracy = np.mean((pred_labels == y_test).cpu().numpy())\n",
    "print('test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: tensor([2, 2, 3, 3, 3, 0, 3, 0, 3, 3, 2, 3, 1, 3, 3, 3, 2, 2, 3, 0],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "real:      tensor([1, 0, 3, 3, 3, 2, 3, 0, 3, 3, 2, 0, 3, 2, 3, 2, 2, 0, 3, 0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# some sample labels:\n",
    "print('predicted: {}'.format(pred_labels[500: 520]))\n",
    "print('real:      {}'.format(y_test[500: 520]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural net for classifying only succesful and not succesful offers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cathegorizing the offers into succesful and not succesful. Labels 0 and 1 correspond to not succesful offers and labels 2 and 3 correspond to succesful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of succesful points: 33598\n",
      "number of succesful points: 27726\n"
     ]
    }
   ],
   "source": [
    "succesful_mask = (training_data_df.label > 1.5)\n",
    "print('number of succesful points: {}'.format(succesful_mask.sum()))\n",
    "print('number of succesful points: {}'.format(len(succesful_mask.values) - succesful_mask.sum()))\n",
    "\n",
    "y = succesful_mask.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size: (49671, 25)\n",
      "validation size: (5520, 25)\n",
      "test size: (6133, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, shuffle=True)\n",
    "print('training size: {}'.format(X_train.shape))\n",
    "print('validation size: {}'.format(X_val.shape))\n",
    "print('test size: {}'.format(X_test.shape))\n",
    "\n",
    "# converting the data into torch.tensor:\n",
    "X_train = torch.as_tensor(X_train, dtype=torch.float).to(device)\n",
    "X_val = torch.as_tensor(X_val, dtype=torch.float).to(device)\n",
    "X_test = torch.as_tensor(X_test, dtype=torch.float).to(device)\n",
    "\n",
    "y_train = torch.as_tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val = torch.as_tensor(y_val, dtype=torch.long).to(device)\n",
    "y_test = torch.as_tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# creating datasets for the dataloaders:\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 2048\n",
    "# creating the data loaders:\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining and training network for binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters:\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = 2\n",
    "hidden_dims = [256, 512, 512, 128]\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING\n",
      "Epoch 0/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 0/99 TRAIN loss/acc : 0.348/53.49%\n",
      "EPOCH 0/99 VAL loss/acc : 0.373/55.16%\n",
      "----------\n",
      "Epoch 1/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 1/99 TRAIN loss/acc : 0.345/55.30%\n",
      "EPOCH 1/99 VAL loss/acc : 0.370/56.70%\n",
      "----------\n",
      "Epoch 2/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 2/99 TRAIN loss/acc : 0.342/56.95%\n",
      "EPOCH 2/99 VAL loss/acc : 0.365/62.32%\n",
      "----------\n",
      "Epoch 3/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 3/99 TRAIN loss/acc : 0.339/59.59%\n",
      "EPOCH 3/99 VAL loss/acc : 0.358/64.26%\n",
      "----------\n",
      "Epoch 4/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 4/99 TRAIN loss/acc : 0.335/60.97%\n",
      "EPOCH 4/99 VAL loss/acc : 0.353/64.55%\n",
      "----------\n",
      "Epoch 5/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 5/99 TRAIN loss/acc : 0.333/61.44%\n",
      "EPOCH 5/99 VAL loss/acc : 0.350/64.80%\n",
      "----------\n",
      "Epoch 6/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 6/99 TRAIN loss/acc : 0.332/61.46%\n",
      "EPOCH 6/99 VAL loss/acc : 0.350/64.67%\n",
      "----------\n",
      "Epoch 7/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 7/99 TRAIN loss/acc : 0.331/61.96%\n",
      "EPOCH 7/99 VAL loss/acc : 0.348/65.09%\n",
      "----------\n",
      "Epoch 8/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 8/99 TRAIN loss/acc : 0.330/61.89%\n",
      "EPOCH 8/99 VAL loss/acc : 0.349/65.14%\n",
      "----------\n",
      "Epoch 9/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 9/99 TRAIN loss/acc : 0.329/62.22%\n",
      "EPOCH 9/99 VAL loss/acc : 0.348/65.00%\n",
      "----------\n",
      "Epoch 10/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 10/99 TRAIN loss/acc : 0.329/62.26%\n",
      "EPOCH 10/99 VAL loss/acc : 0.348/65.07%\n",
      "----------\n",
      "Epoch 11/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 11/99 TRAIN loss/acc : 0.330/62.15%\n",
      "EPOCH 11/99 VAL loss/acc : 0.348/64.91%\n",
      "----------\n",
      "Epoch 12/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 12/99 TRAIN loss/acc : 0.329/62.09%\n",
      "EPOCH 12/99 VAL loss/acc : 0.348/65.14%\n",
      "----------\n",
      "Epoch 13/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 13/99 TRAIN loss/acc : 0.329/62.12%\n",
      "EPOCH 13/99 VAL loss/acc : 0.348/65.14%\n",
      "----------\n",
      "Epoch 14/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 14/99 TRAIN loss/acc : 0.329/62.22%\n",
      "EPOCH 14/99 VAL loss/acc : 0.348/65.05%\n",
      "----------\n",
      "Epoch 15/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 15/99 TRAIN loss/acc : 0.329/62.20%\n",
      "EPOCH 15/99 VAL loss/acc : 0.348/65.24%\n",
      "----------\n",
      "Epoch 16/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 16/99 TRAIN loss/acc : 0.328/62.35%\n",
      "EPOCH 16/99 VAL loss/acc : 0.347/65.20%\n",
      "----------\n",
      "Epoch 17/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 17/99 TRAIN loss/acc : 0.328/62.50%\n",
      "EPOCH 17/99 VAL loss/acc : 0.347/65.05%\n",
      "----------\n",
      "Epoch 18/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 18/99 TRAIN loss/acc : 0.328/62.71%\n",
      "EPOCH 18/99 VAL loss/acc : 0.347/65.18%\n",
      "----------\n",
      "Epoch 19/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 19/99 TRAIN loss/acc : 0.328/62.66%\n",
      "EPOCH 19/99 VAL loss/acc : 0.348/65.24%\n",
      "----------\n",
      "Epoch 20/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 20/99 TRAIN loss/acc : 0.328/62.43%\n",
      "EPOCH 20/99 VAL loss/acc : 0.347/65.20%\n",
      "----------\n",
      "Epoch 21/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 21/99 TRAIN loss/acc : 0.327/62.80%\n",
      "EPOCH 21/99 VAL loss/acc : 0.347/65.22%\n",
      "----------\n",
      "Epoch 22/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 22/99 TRAIN loss/acc : 0.328/62.44%\n",
      "EPOCH 22/99 VAL loss/acc : 0.347/65.11%\n",
      "----------\n",
      "Epoch 23/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 23/99 TRAIN loss/acc : 0.328/62.50%\n",
      "EPOCH 23/99 VAL loss/acc : 0.347/65.34%\n",
      "----------\n",
      "Epoch 24/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 24/99 TRAIN loss/acc : 0.327/62.42%\n",
      "EPOCH 24/99 VAL loss/acc : 0.346/65.25%\n",
      "----------\n",
      "Epoch 25/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 25/99 TRAIN loss/acc : 0.328/62.57%\n",
      "EPOCH 25/99 VAL loss/acc : 0.347/65.18%\n",
      "----------\n",
      "Epoch 26/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 26/99 TRAIN loss/acc : 0.328/62.55%\n",
      "EPOCH 26/99 VAL loss/acc : 0.347/65.31%\n",
      "----------\n",
      "Epoch 27/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 27/99 TRAIN loss/acc : 0.328/62.49%\n",
      "EPOCH 27/99 VAL loss/acc : 0.347/65.33%\n",
      "----------\n",
      "Epoch 28/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 28/99 TRAIN loss/acc : 0.327/62.60%\n",
      "EPOCH 28/99 VAL loss/acc : 0.346/65.31%\n",
      "----------\n",
      "Epoch 29/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 29/99 TRAIN loss/acc : 0.327/62.67%\n",
      "EPOCH 29/99 VAL loss/acc : 0.347/65.25%\n",
      "----------\n",
      "Epoch 30/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 30/99 TRAIN loss/acc : 0.327/62.93%\n",
      "EPOCH 30/99 VAL loss/acc : 0.346/65.27%\n",
      "----------\n",
      "Epoch 31/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 31/99 TRAIN loss/acc : 0.328/62.55%\n",
      "EPOCH 31/99 VAL loss/acc : 0.346/65.56%\n",
      "----------\n",
      "Epoch 32/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 32/99 TRAIN loss/acc : 0.327/62.66%\n",
      "EPOCH 32/99 VAL loss/acc : 0.346/65.42%\n",
      "----------\n",
      "Epoch 33/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 33/99 TRAIN loss/acc : 0.327/62.72%\n",
      "EPOCH 33/99 VAL loss/acc : 0.346/65.58%\n",
      "----------\n",
      "Epoch 34/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 34/99 TRAIN loss/acc : 0.327/62.46%\n",
      "EPOCH 34/99 VAL loss/acc : 0.346/65.47%\n",
      "----------\n",
      "Epoch 35/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 35/99 TRAIN loss/acc : 0.327/62.77%\n",
      "EPOCH 35/99 VAL loss/acc : 0.346/65.47%\n",
      "----------\n",
      "Epoch 36/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 36/99 TRAIN loss/acc : 0.327/62.52%\n",
      "EPOCH 36/99 VAL loss/acc : 0.346/65.51%\n",
      "----------\n",
      "Epoch 37/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 37/99 TRAIN loss/acc : 0.327/62.77%\n",
      "EPOCH 37/99 VAL loss/acc : 0.346/65.51%\n",
      "----------\n",
      "Epoch 38/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 38/99 TRAIN loss/acc : 0.327/62.73%\n",
      "EPOCH 38/99 VAL loss/acc : 0.346/65.40%\n",
      "----------\n",
      "Epoch 39/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 39/99 TRAIN loss/acc : 0.327/62.76%\n",
      "EPOCH 39/99 VAL loss/acc : 0.346/65.51%\n",
      "----------\n",
      "Epoch 40/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 40/99 TRAIN loss/acc : 0.327/62.87%\n",
      "EPOCH 40/99 VAL loss/acc : 0.346/65.71%\n",
      "----------\n",
      "Epoch 41/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 41/99 TRAIN loss/acc : 0.327/62.68%\n",
      "EPOCH 41/99 VAL loss/acc : 0.345/65.78%\n",
      "----------\n",
      "Epoch 42/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 42/99 TRAIN loss/acc : 0.327/62.65%\n",
      "EPOCH 42/99 VAL loss/acc : 0.345/65.74%\n",
      "----------\n",
      "Epoch 43/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 43/99 TRAIN loss/acc : 0.327/62.53%\n",
      "EPOCH 43/99 VAL loss/acc : 0.346/65.49%\n",
      "----------\n",
      "Epoch 44/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 44/99 TRAIN loss/acc : 0.327/62.71%\n",
      "EPOCH 44/99 VAL loss/acc : 0.345/65.65%\n",
      "----------\n",
      "Epoch 45/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 45/99 TRAIN loss/acc : 0.326/62.80%\n",
      "EPOCH 45/99 VAL loss/acc : 0.346/65.71%\n",
      "----------\n",
      "Epoch 46/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 46/99 TRAIN loss/acc : 0.326/62.97%\n",
      "EPOCH 46/99 VAL loss/acc : 0.346/65.40%\n",
      "----------\n",
      "Epoch 47/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 47/99 TRAIN loss/acc : 0.327/62.82%\n",
      "EPOCH 47/99 VAL loss/acc : 0.346/65.53%\n",
      "----------\n",
      "Epoch 48/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 48/99 TRAIN loss/acc : 0.327/62.80%\n",
      "EPOCH 48/99 VAL loss/acc : 0.346/65.53%\n",
      "----------\n",
      "Epoch 49/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 49/99 TRAIN loss/acc : 0.327/62.58%\n",
      "EPOCH 49/99 VAL loss/acc : 0.346/65.71%\n",
      "----------\n",
      "Epoch 50/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 50/99 TRAIN loss/acc : 0.327/62.94%\n",
      "EPOCH 50/99 VAL loss/acc : 0.346/65.47%\n",
      "----------\n",
      "Epoch 51/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 51/99 TRAIN loss/acc : 0.327/62.82%\n",
      "EPOCH 51/99 VAL loss/acc : 0.345/65.51%\n",
      "----------\n",
      "Epoch 52/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 52/99 TRAIN loss/acc : 0.327/62.94%\n",
      "EPOCH 52/99 VAL loss/acc : 0.346/65.56%\n",
      "----------\n",
      "Epoch 53/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 53/99 TRAIN loss/acc : 0.327/62.64%\n",
      "EPOCH 53/99 VAL loss/acc : 0.346/65.42%\n",
      "----------\n",
      "Epoch 54/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 54/99 TRAIN loss/acc : 0.326/62.78%\n",
      "EPOCH 54/99 VAL loss/acc : 0.346/65.65%\n",
      "----------\n",
      "Epoch 55/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 55/99 TRAIN loss/acc : 0.327/62.68%\n",
      "EPOCH 55/99 VAL loss/acc : 0.346/65.71%\n",
      "----------\n",
      "Epoch 56/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 56/99 TRAIN loss/acc : 0.327/62.57%\n",
      "EPOCH 56/99 VAL loss/acc : 0.346/65.60%\n",
      "----------\n",
      "Epoch 57/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 57/99 TRAIN loss/acc : 0.326/62.88%\n",
      "EPOCH 57/99 VAL loss/acc : 0.346/65.74%\n",
      "----------\n",
      "Epoch 58/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 58/99 TRAIN loss/acc : 0.326/62.70%\n",
      "EPOCH 58/99 VAL loss/acc : 0.346/65.65%\n",
      "----------\n",
      "Epoch 59/99\n",
      "learning rate : 0.0001\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 59/99 TRAIN loss/acc : 0.327/62.75%\n",
      "EPOCH 59/99 VAL loss/acc : 0.346/65.74%\n",
      "----------\n",
      "Epoch 60/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 60/99 TRAIN loss/acc : 0.326/62.95%\n",
      "EPOCH 60/99 VAL loss/acc : 0.345/65.54%\n",
      "----------\n",
      "Epoch 61/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 61/99 TRAIN loss/acc : 0.326/62.89%\n",
      "EPOCH 61/99 VAL loss/acc : 0.345/65.76%\n",
      "----------\n",
      "Epoch 62/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 62/99 TRAIN loss/acc : 0.326/62.94%\n",
      "EPOCH 62/99 VAL loss/acc : 0.345/65.71%\n",
      "----------\n",
      "Epoch 63/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 63/99 TRAIN loss/acc : 0.327/62.82%\n",
      "EPOCH 63/99 VAL loss/acc : 0.345/65.72%\n",
      "----------\n",
      "Epoch 64/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 64/99 TRAIN loss/acc : 0.326/62.88%\n",
      "EPOCH 64/99 VAL loss/acc : 0.346/65.62%\n",
      "----------\n",
      "Epoch 65/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 65/99 TRAIN loss/acc : 0.326/63.02%\n",
      "EPOCH 65/99 VAL loss/acc : 0.345/65.51%\n",
      "----------\n",
      "Epoch 66/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 66/99 TRAIN loss/acc : 0.326/63.06%\n",
      "EPOCH 66/99 VAL loss/acc : 0.344/65.76%\n",
      "----------\n",
      "Epoch 67/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 67/99 TRAIN loss/acc : 0.326/63.02%\n",
      "EPOCH 67/99 VAL loss/acc : 0.345/65.47%\n",
      "----------\n",
      "Epoch 68/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 68/99 TRAIN loss/acc : 0.326/62.87%\n",
      "EPOCH 68/99 VAL loss/acc : 0.346/65.76%\n",
      "----------\n",
      "Epoch 69/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 69/99 TRAIN loss/acc : 0.327/62.89%\n",
      "EPOCH 69/99 VAL loss/acc : 0.345/65.60%\n",
      "----------\n",
      "Epoch 70/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 70/99 TRAIN loss/acc : 0.326/62.71%\n",
      "EPOCH 70/99 VAL loss/acc : 0.346/65.78%\n",
      "----------\n",
      "Epoch 71/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 71/99 TRAIN loss/acc : 0.327/62.87%\n",
      "EPOCH 71/99 VAL loss/acc : 0.345/65.71%\n",
      "----------\n",
      "Epoch 72/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 72/99 TRAIN loss/acc : 0.326/62.94%\n",
      "EPOCH 72/99 VAL loss/acc : 0.345/65.98%\n",
      "----------\n",
      "Epoch 73/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 73/99 TRAIN loss/acc : 0.326/62.91%\n",
      "EPOCH 73/99 VAL loss/acc : 0.345/65.74%\n",
      "----------\n",
      "Epoch 74/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 74/99 TRAIN loss/acc : 0.326/62.89%\n",
      "EPOCH 74/99 VAL loss/acc : 0.346/65.49%\n",
      "----------\n",
      "Epoch 75/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 75/99 TRAIN loss/acc : 0.326/62.93%\n",
      "EPOCH 75/99 VAL loss/acc : 0.345/65.78%\n",
      "----------\n",
      "Epoch 76/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 76/99 TRAIN loss/acc : 0.326/63.05%\n",
      "EPOCH 76/99 VAL loss/acc : 0.345/65.78%\n",
      "----------\n",
      "Epoch 77/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 77/99 TRAIN loss/acc : 0.327/62.82%\n",
      "EPOCH 77/99 VAL loss/acc : 0.346/65.78%\n",
      "----------\n",
      "Epoch 78/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 78/99 TRAIN loss/acc : 0.325/62.99%\n",
      "EPOCH 78/99 VAL loss/acc : 0.346/65.63%\n",
      "----------\n",
      "Epoch 79/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 79/99 TRAIN loss/acc : 0.326/62.96%\n",
      "EPOCH 79/99 VAL loss/acc : 0.345/65.87%\n",
      "----------\n",
      "Epoch 80/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 80/99 TRAIN loss/acc : 0.326/62.83%\n",
      "EPOCH 80/99 VAL loss/acc : 0.345/65.83%\n",
      "----------\n",
      "Epoch 81/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 81/99 TRAIN loss/acc : 0.326/63.02%\n",
      "EPOCH 81/99 VAL loss/acc : 0.345/65.58%\n",
      "----------\n",
      "Epoch 82/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 82/99 TRAIN loss/acc : 0.325/63.04%\n",
      "EPOCH 82/99 VAL loss/acc : 0.346/65.80%\n",
      "----------\n",
      "Epoch 83/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 83/99 TRAIN loss/acc : 0.326/62.92%\n",
      "EPOCH 83/99 VAL loss/acc : 0.346/65.94%\n",
      "----------\n",
      "Epoch 84/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 84/99 TRAIN loss/acc : 0.326/63.07%\n",
      "EPOCH 84/99 VAL loss/acc : 0.345/65.74%\n",
      "----------\n",
      "Epoch 85/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 85/99 TRAIN loss/acc : 0.326/62.92%\n",
      "EPOCH 85/99 VAL loss/acc : 0.345/65.94%\n",
      "----------\n",
      "Epoch 86/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 86/99 TRAIN loss/acc : 0.326/62.81%\n",
      "EPOCH 86/99 VAL loss/acc : 0.346/65.74%\n",
      "----------\n",
      "Epoch 87/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 87/99 TRAIN loss/acc : 0.325/62.96%\n",
      "EPOCH 87/99 VAL loss/acc : 0.345/66.00%\n",
      "----------\n",
      "Epoch 88/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 88/99 TRAIN loss/acc : 0.326/63.08%\n",
      "EPOCH 88/99 VAL loss/acc : 0.346/65.76%\n",
      "----------\n",
      "Epoch 89/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 89/99 TRAIN loss/acc : 0.326/62.92%\n",
      "EPOCH 89/99 VAL loss/acc : 0.345/65.87%\n",
      "----------\n",
      "Epoch 90/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 90/99 TRAIN loss/acc : 0.326/62.91%\n",
      "EPOCH 90/99 VAL loss/acc : 0.346/65.72%\n",
      "----------\n",
      "Epoch 91/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 91/99 TRAIN loss/acc : 0.326/63.02%\n",
      "EPOCH 91/99 VAL loss/acc : 0.345/65.62%\n",
      "----------\n",
      "Epoch 92/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 92/99 TRAIN loss/acc : 0.326/63.02%\n",
      "EPOCH 92/99 VAL loss/acc : 0.345/65.85%\n",
      "----------\n",
      "Epoch 93/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 93/99 TRAIN loss/acc : 0.326/62.80%\n",
      "EPOCH 93/99 VAL loss/acc : 0.345/65.82%\n",
      "----------\n",
      "Epoch 94/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 94/99 TRAIN loss/acc : 0.325/63.10%\n",
      "EPOCH 94/99 VAL loss/acc : 0.345/65.83%\n",
      "----------\n",
      "Epoch 95/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 95/99 TRAIN loss/acc : 0.326/62.91%\n",
      "EPOCH 95/99 VAL loss/acc : 0.345/65.87%\n",
      "----------\n",
      "Epoch 96/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 96/99 TRAIN loss/acc : 0.326/63.04%\n",
      "EPOCH 96/99 VAL loss/acc : 0.345/65.72%\n",
      "----------\n",
      "Epoch 97/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 97/99 TRAIN loss/acc : 0.326/62.99%\n",
      "EPOCH 97/99 VAL loss/acc : 0.345/65.82%\n",
      "----------\n",
      "Epoch 98/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 98/99 TRAIN loss/acc : 0.326/63.06%\n",
      "EPOCH 98/99 VAL loss/acc : 0.345/65.89%\n",
      "----------\n",
      "Epoch 99/99\n",
      "learning rate : 0.0001\n",
      "----------\n",
      "EPOCH 99/99 TRAIN loss/acc : 0.326/63.12%\n",
      "EPOCH 99/99 VAL loss/acc : 0.346/65.69%\n",
      "----------\n",
      "FINISH.\n",
      "Training complete in 1m 27s\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model:\n",
    "model_NN_binary = Linear_NN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim,\n",
    "                     dropout=dropout, activation='leaky_relu')\n",
    "model_NN_binary.to(device)\n",
    "\n",
    "saving_path = 'models/NN_model_binary.pth'\n",
    "\n",
    "NN_solver_binary = NN_Solver(optim_args={\"lr\": 1e-4, \"weight_decay\": 0},\n",
    "                loss_func=torch.nn.CrossEntropyLoss())\n",
    "NN_solver_binary.train(model_NN_binary, train_loader, val_loader,\n",
    "                       log_nth=0, num_epochs=100, saving_path=saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy: 0.659963768115942\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing the train and validation loss history:\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(NN_solver_binary.train_loss_history)\n",
    "plt.plot(NN_solver_binary.val_loss_history)\n",
    "\n",
    "print('best validation accuracy: {}'.format(max(NN_solver_binary.val_acc_history)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "After the accuracy of the binary classifier the model can be tested on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.6606880808739606\n"
     ]
    }
   ],
   "source": [
    "# testing the trained model:\n",
    "model_NN_binary.eval()\n",
    "output = model_NN_binary(X_test)\n",
    "pred_labels = output.argmax(dim=1)\n",
    "\n",
    "test_accuracy = np.mean((pred_labels == y_test).cpu().numpy())\n",
    "print('test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: tensor([1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "real:      tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# some sample labels:\n",
    "print('predicted: {}'.format(pred_labels[500: 520]))\n",
    "print('real:      {}'.format(y_test[500: 520]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('starbucks': conda)",
   "language": "python",
   "name": "python38264bitstarbuckscondad9e38c9f842c41baadfb9dbf2b751888"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
